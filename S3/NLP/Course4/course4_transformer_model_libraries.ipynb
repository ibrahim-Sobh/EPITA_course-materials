{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"course4_transformer_model_libraries.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"pycharm":{"stem_cell":{"cell_type":"raw","metadata":{"collapsed":false},"source":[]}},"widgets":{"application/vnd.jupyter.widget-state+json":{"1e5d9c11d79847178ca91096a9e4bf97":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b7225e33cdac461fb2ff79cba9f9e0ba","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3ee11cbb316346369b1cb6284739776b","IPY_MODEL_94d1f3778f9641a0ba9bf7cb9e6232bc"]}},"b7225e33cdac461fb2ff79cba9f9e0ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3ee11cbb316346369b1cb6284739776b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a76258acf127428b8b887f7739e63a5a","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":526681800,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":526681800,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7a4acefb12324bcc8961b47cec545628"}},"94d1f3778f9641a0ba9bf7cb9e6232bc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c8ab0e7da75b47c7a44cc6344a57069c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 527M/527M [00:13&lt;00:00, 38.4MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e9ed9d03b5d24e1b8b8167c495b4c6b2"}},"a76258acf127428b8b887f7739e63a5a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7a4acefb12324bcc8961b47cec545628":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c8ab0e7da75b47c7a44cc6344a57069c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e9ed9d03b5d24e1b8b8167c495b4c6b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5f9097f4567f4bbc857aa8c82e4db738":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c98b128284f44ca1b01a73908fae5048","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e79a9e3733d2401bb4cfe17385bd2c37","IPY_MODEL_52ae5d5c0cb64879ba031160f56f9436"]}},"c98b128284f44ca1b01a73908fae5048":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e79a9e3733d2401bb4cfe17385bd2c37":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_869ed6bc7c6b443fa8ddb5f46d35570a","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":411,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":411,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_011eb3aab41e4b949e955532d5743c6f"}},"52ae5d5c0cb64879ba031160f56f9436":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_71beaec2fa9946c396f8d431a47ce653","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 411/411 [00:00&lt;00:00, 1.43kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_38340fe884d54d3795aea136fb629197"}},"869ed6bc7c6b443fa8ddb5f46d35570a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"011eb3aab41e4b949e955532d5743c6f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"71beaec2fa9946c396f8d431a47ce653":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"38340fe884d54d3795aea136fb629197":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a96481187e7d43338e6b57c1ef028704":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ebe977fc97f6404db1419df62155d93d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ef3f0470bd4c4703a540aeff8b195376","IPY_MODEL_e884e5db4a8348c9b2bbe768384ddb38"]}},"ebe977fc97f6404db1419df62155d93d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ef3f0470bd4c4703a540aeff8b195376":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_19540dc05f91476883bc2ef0fcba770c","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":263273408,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":263273408,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_81c23f3b17344c039feb6c672a613c8f"}},"e884e5db4a8348c9b2bbe768384ddb38":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_665edb82b1634ec49fd3f0975a9ec8f0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 263M/263M [00:21&lt;00:00, 12.2MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c8ac18c961b74ce890a32cf31f6b86ce"}},"19540dc05f91476883bc2ef0fcba770c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"81c23f3b17344c039feb6c672a613c8f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"665edb82b1634ec49fd3f0975a9ec8f0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c8ac18c961b74ce890a32cf31f6b86ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"be4f5c195a024264ac043e91225a2123":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6e8820e81fb94f77875bdcdc2d6353eb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_703cefb637164b2dabb4e1048d0bb1ce","IPY_MODEL_cfa50509fbdc4a529049279185f177c7"]}},"6e8820e81fb94f77875bdcdc2d6353eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"703cefb637164b2dabb4e1048d0bb1ce":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1d01eedd955b4a9ba270a57cded26a75","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":456,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":456,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cdfb36379ad34f4d99b4c5c82f2bd12f"}},"cfa50509fbdc4a529049279185f177c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0ca4396c1653446681667e3a4c7d9010","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 456/456 [00:00&lt;00:00, 1.80kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8ad1c98c6e424a9eacb14a9e41395231"}},"1d01eedd955b4a9ba270a57cded26a75":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"cdfb36379ad34f4d99b4c5c82f2bd12f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0ca4396c1653446681667e3a4c7d9010":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8ad1c98c6e424a9eacb14a9e41395231":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d9e33ac6a1054bbbb6e65bf0a84d2a30":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0f0f993d2c19441ba63c1a61d3e5d74a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_80a1b97710e34d97950091831d0e1ff4","IPY_MODEL_bda6cfd394a94033860cf9ec3d697449"]}},"0f0f993d2c19441ba63c1a61d3e5d74a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"80a1b97710e34d97950091831d0e1ff4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a213b02300d2479dacaace70f505e542","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":442252390,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":442252390,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9a9e37c19cd2452c82da0fcf65c0419f"}},"bda6cfd394a94033860cf9ec3d697449":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b7f049fa0a7e42e9b50e092d5c55caa0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 442M/442M [00:12&lt;00:00, 36.8MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d76aa60247f94d32866a29d0c0df5f17"}},"a213b02300d2479dacaace70f505e542":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9a9e37c19cd2452c82da0fcf65c0419f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b7f049fa0a7e42e9b50e092d5c55caa0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d76aa60247f94d32866a29d0c0df5f17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"974925b377cc43a8b43ca188810b00c7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8edbdcc86f284db99b3f223a9faa478c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d5039e3e39c6414886c96bfb54511500","IPY_MODEL_76bb044a039d46d2994b38883746b70d"]}},"8edbdcc86f284db99b3f223a9faa478c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d5039e3e39c6414886c96bfb54511500":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_953290850c8e42d78172812440c9b0a9","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":239836,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":239836,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c33470fbcc5947a1ab79eeae5a3038f7"}},"76bb044a039d46d2994b38883746b70d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9942063cdf6a4a499526f41a1377f838","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 240k/240k [00:00&lt;00:00, 682kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f48704508cd043d3bdca8babfa34d18d"}},"953290850c8e42d78172812440c9b0a9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c33470fbcc5947a1ab79eeae5a3038f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9942063cdf6a4a499526f41a1377f838":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f48704508cd043d3bdca8babfa34d18d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"42731d52f5414f1f8dfcd919946bc1a7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6c3d8f8451704c198653b42e9c196233","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ac62e8db919e499b8b64642cf173efb8","IPY_MODEL_4cbc41b07abd4fcdb45ea930d574ada6"]}},"6c3d8f8451704c198653b42e9c196233":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ac62e8db919e499b8b64642cf173efb8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_280af4a878d940b683d541a7d29a5bbf","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":59,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":59,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_182ea22e1df54abbbaeb631b54298c57"}},"4cbc41b07abd4fcdb45ea930d574ada6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_93e38e0997b74de79e0b4e615a325588","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 59.0/59.0 [00:00&lt;00:00, 564B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_36d2cccef5b44f8caf52c8fb19be7c6d"}},"280af4a878d940b683d541a7d29a5bbf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"182ea22e1df54abbbaeb631b54298c57":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"93e38e0997b74de79e0b4e615a325588":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"36d2cccef5b44f8caf52c8fb19be7c6d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"_4aQSfDl07qq"},"source":["## Disclaimer:\n","**This notebook comes from https://colab.research.google.com/github/huggingface/transformers/blob/master/notebooks/02-transformers.ipynb#scrollTo=yAwN5gQcEqYv**"]},{"cell_type":"markdown","metadata":{"id":"YKdSeUmVSXah","pycharm":{"is_executing":false,"name":"#%% md\n"}},"source":["## Introduction\n","The transformers library is an open-source, community-based repository to train, use and share models based on \n","the Transformer architecture [(Vaswani & al., 2017)](https://arxiv.org/abs/1706.03762) such as Bert [(Devlin & al., 2018)](https://arxiv.org/abs/1810.04805),\n","Roberta [(Liu & al., 2019)](https://arxiv.org/abs/1907.11692), GPT2 [(Radford & al., 2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf),\n","XLNet [(Yang & al., 2019)](https://arxiv.org/abs/1906.08237), etc. \n","\n","Along with the models, the library contains multiple variations of each of them for a large variety of \n","downstream-tasks like **Named Entity Recognition (NER)**, **Sentiment Analysis**, \n","**Language Modeling**, **Question Answering** and so on.\n","\n","## Before Transformer\n","\n","Back to 2017, most of the people using Neural Networks when working on Natural Language Processing were relying on \n","sequential processing of the input through [Recurrent Neural Network (RNN)](https://en.wikipedia.org/wiki/Recurrent_neural_network).\n","\n","![rnn](http://colah.github.io/posts/2015-09-NN-Types-FP/img/RNN-general.png)   \n","\n","RNNs were performing well on large variety of tasks involving sequential dependency over the input sequence. \n","However, this sequentially-dependent process had issues modeling very long range dependencies and \n","was not well suited for the kind of hardware we're currently leveraging due to bad parallelization capabilities. \n","\n","Some extensions were provided by the academic community, such as Bidirectional RNN ([Schuster & Paliwal., 1997](https://www.researchgate.net/publication/3316656_Bidirectional_recurrent_neural_networks), [Graves & al., 2005](https://mediatum.ub.tum.de/doc/1290195/file.pdf)), \n","which can be seen as a concatenation of two sequential process, one going forward, the other one going backward over the sequence input.\n","\n","![birnn](https://miro.medium.com/max/764/1*6QnPUSv_t9BY9Fv8_aLb-Q.png)\n","\n","\n","And also, the Attention mechanism, which introduced a good improvement over \"raw\" RNNs by giving \n","a learned, weighted-importance to each element in the sequence, allowing the model to focus on important elements.\n","\n","![attention_rnn](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/08/Example-of-Attention.png)  \n","\n","## Then comes the Transformer  \n","\n","The Transformers era originally started from the work of [(Vaswani & al., 2017)](https://arxiv.org/abs/1706.03762) who\n","demonstrated its superiority over [Recurrent Neural Network (RNN)](https://en.wikipedia.org/wiki/Recurrent_neural_network)\n","on translation tasks but it quickly extended to almost all the tasks RNNs were State-of-the-Art at that time.\n","\n","One advantage of Transformer over its RNN counterpart was its non sequential attention model. Remember, the RNNs had to\n","iterate over each element of the input sequence one-by-one and carry an \"updatable-state\" between each hop. With Transformer, the model is able to look at every position in the sequence, at the same time, in one operation.\n","\n","For a deep-dive into the Transformer architecture, [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html#encoder-and-decoder-stacks) \n","will drive you along all the details of the paper.\n","\n","![transformer-encoder-decoder](https://nlp.seas.harvard.edu/images/the-annotated-transformer_14_0.png)"]},{"cell_type":"markdown","metadata":{"id":"TFHTP6CFSXai","pycharm":{"name":"#%% md\n"}},"source":["## Getting started with transformers\n","\n","For the rest of this notebook, we will use the [BERT (Devlin & al., 2018)](https://arxiv.org/abs/1810.04805) architecture, as it's the most simple and there are plenty of content about it\n","over the internet, it will be easy to dig more over this architecture if you want to.\n","\n","The transformers library allows you to benefits from large, pretrained language models without requiring a huge and costly computational\n","infrastructure. Most of the State-of-the-Art models are provided directly by their author and made available in the library \n","in PyTorch and TensorFlow in a transparent and interchangeable way. \n","\n","If you're executing this notebook in Colab, you will need to install the transformers library. You can do so with this command:"]},{"cell_type":"code","metadata":{"id":"KnT3Jn6fSXai","pycharm":{"is_executing":false,"name":"#%% code\n"},"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9f74a684-da23-4d65-a87e-a06f3d7778c1"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.7.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UIQGDTIDSXai","pycharm":{"is_executing":false,"name":"#%% code\n"},"outputId":"f0de3111-d230-402e-baa2-f4499fb80349"},"source":["import torch\n","from transformers import AutoModel, AutoTokenizer, BertTokenizer\n","\n","torch.set_grad_enabled(False)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.autograd.grad_mode.set_grad_enabled at 0x7f0098fb4510>"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"1xMDTHQXSXai","pycharm":{"is_executing":false,"name":"#%% code\n"},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f6756d1c-12f3-4b4d-b6aa-dc6cb7b9f62c"},"source":["# Store the model we want to use\n","MODEL_NAME = \"bert-base-cased\"\n","\n","# We need to create the model and tokenizer\n","model = AutoModel.from_pretrained(MODEL_NAME)\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"l6EcynhYSXai","pycharm":{"name":"#%% md\n"}},"source":["With only the above two lines of code, you're ready to use a BERT pre-trained model. \n","The tokenizers will allow us to map a raw textual input to a sequence of integers representing our textual input\n","in a way the model can manipulate. Since we will be using a PyTorch model, we ask the tokenizer to return to us PyTorch tensors."]},{"cell_type":"code","metadata":{"id":"yAwN5gQcEqYv","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8194441f-76f1-419d-fbc3-a378da551e69"},"source":["tokens_pt = tokenizer(\"This is an input example\", return_tensors=\"pt\")\n","for key, value in tokens_pt.items():\n","    print(\"{}:\\n\\t{}\".format(key, value))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["input_ids:\n","\ttensor([[ 101, 1188, 1110, 1126, 7758, 1859,  102]])\n","token_type_ids:\n","\ttensor([[0, 0, 0, 0, 0, 0, 0]])\n","attention_mask:\n","\ttensor([[1, 1, 1, 1, 1, 1, 1]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"z88teFyGEqYw"},"source":["The tokenizer automatically converted our input to all the inputs expected by the model. It generated some additional tensors on top of the IDs: \n","\n","- token_type_ids: This tensor will map every tokens to their corresponding segment (see below).\n","- attention_mask: This tensor is used to \"mask\" padded values in a batch of sequence with different lengths (see below).\n","\n","You can check our [glossary](https://huggingface.co/transformers/glossary.html) for more information about each of those keys. \n","\n","We can just feed this directly into our model:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XgkFg52fSXai","pycharm":{"is_executing":false,"name":"#%% code\n"},"outputId":"11881bcd-46fb-4a5d-ddc7-26e0dda3ca8f"},"source":["outputs = model(**tokens_pt)\n","last_hidden_state = outputs.last_hidden_state\n","pooler_output = outputs.pooler_output\n","\n","print(\"Token wise output: {}, Pooled output: {}\".format(last_hidden_state.shape, pooler_output.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Token wise output: torch.Size([1, 7, 768]), Pooled output: torch.Size([1, 768])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lBbvwNKXSXaj","pycharm":{"name":"#%% md\n"}},"source":["As you can see, BERT outputs two tensors:\n"," - One with the generated representation for every token in the input `(1, NB_TOKENS, REPRESENTATION_SIZE)`\n"," - One with an aggregated representation for the whole input `(1, REPRESENTATION_SIZE)`\n"," \n","The first, token-based, representation can be leveraged if your task requires to keep the sequence representation and you\n","want to operate at a token-level. This is particularly useful for Named Entity Recognition and Question-Answering.\n","\n","The second, aggregated, representation is especially useful if you need to extract the overall context of the sequence and don't\n","require a fine-grained token-level. This is the case for Sentiment-Analysis of the sequence or Information Retrieval."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pl2HIcwDSXal","pycharm":{"is_executing":false},"outputId":"b862167a-fa54-4087-a415-5cc452791359"},"source":["# Single segment input\n","single_seg_input = tokenizer(\"This is a sample input\")\n","\n","# Multiple segment input\n","multi_seg_input = tokenizer(\"This is segment A\", \"This is segment B\")\n","\n","print(\"Single segment token (str): {}\".format(tokenizer.convert_ids_to_tokens(single_seg_input['input_ids'])))\n","print(\"Single segment token (int): {}\".format(single_seg_input['input_ids']))\n","print(\"Single segment type       : {}\".format(single_seg_input['token_type_ids']))\n","\n","# Segments are concatened in the input to the model, with \n","print()\n","print(\"Multi segment token (str): {}\".format(tokenizer.convert_ids_to_tokens(multi_seg_input['input_ids'])))\n","print(\"Multi segment token (int): {}\".format(multi_seg_input['input_ids']))\n","print(\"Multi segment type       : {}\".format(multi_seg_input['token_type_ids']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Single segment token (str): ['[CLS]', 'This', 'is', 'a', 'sample', 'input', '[SEP]']\n","Single segment token (int): [101, 1188, 1110, 170, 6876, 7758, 102]\n","Single segment type       : [0, 0, 0, 0, 0, 0, 0]\n","\n","Multi segment token (str): ['[CLS]', 'This', 'is', 'segment', 'A', '[SEP]', 'This', 'is', 'segment', 'B', '[SEP]']\n","Multi segment token (int): [101, 1188, 1110, 6441, 138, 102, 1188, 1110, 6441, 139, 102]\n","Multi segment type       : [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1NtvWOgzSXam","pycharm":{"is_executing":false},"outputId":"e362ddc9-f546-4c99-d7a8-9d91658273df"},"source":["# Padding highlight\n","tokens = tokenizer(\n","    [\"This is a sample\", \"This is another longer sample text\"], \n","    padding=True  # First sentence will have some PADDED tokens to match second sequence length\n",")\n","\n","for i in range(2):\n","    print(\"Tokens (int)      : {}\".format(tokens['input_ids'][i]))\n","    print(\"Tokens (str)      : {}\".format([tokenizer.convert_ids_to_tokens(s) for s in tokens['input_ids'][i]]))\n","    print(\"Tokens (attn_mask): {}\".format(tokens['attention_mask'][i]))\n","    print()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tokens (int)      : [101, 1188, 1110, 170, 6876, 102, 0, 0]\n","Tokens (str)      : ['[CLS]', 'This', 'is', 'a', 'sample', '[SEP]', '[PAD]', '[PAD]']\n","Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 0, 0]\n","\n","Tokens (int)      : [101, 1188, 1110, 1330, 2039, 6876, 3087, 102]\n","Tokens (str)      : ['[CLS]', 'This', 'is', 'another', 'longer', 'sample', 'text', '[SEP]']\n","Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1, 1]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vkRYm2HESXan"},"source":["## Frameworks interoperability\n","\n","One of the most powerfull feature of transformers is its ability to seamlessly move from PyTorch to Tensorflow\n","without pain for the user.\n","\n","We provide some convenient methods to load TensorFlow pretrained weight insinde a PyTorch model and opposite."]},{"cell_type":"code","metadata":{"id":"Kubwm-wJSXan","pycharm":{"is_executing":false},"colab":{"base_uri":"https://localhost:8080/","height":238,"referenced_widgets":["1e5d9c11d79847178ca91096a9e4bf97","b7225e33cdac461fb2ff79cba9f9e0ba","3ee11cbb316346369b1cb6284739776b","94d1f3778f9641a0ba9bf7cb9e6232bc","a76258acf127428b8b887f7739e63a5a","7a4acefb12324bcc8961b47cec545628","c8ab0e7da75b47c7a44cc6344a57069c","e9ed9d03b5d24e1b8b8167c495b4c6b2"]},"outputId":"65acce3f-e79c-4e8f-a433-2699c957890a"},"source":["from transformers import TFBertModel, BertModel\n","\n","# Let's load a BERT model for TensorFlow and PyTorch\n","model_tf = TFBertModel.from_pretrained('bert-base-cased')\n","model_pt = BertModel.from_pretrained('bert-base-cased')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1e5d9c11d79847178ca91096a9e4bf97","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=526681800.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lJ13tlzOSXan","pycharm":{"is_executing":false},"outputId":"610052b3-56ee-488f-9de8-fac0fb1ce565"},"source":["# transformers generates a ready to use dictionary with all the required parameters for the specific framework.\n","input_tf = tokenizer(\"This is a sample input\", return_tensors=\"tf\")\n","input_pt = tokenizer(\"This is a sample input\", return_tensors=\"pt\")\n","\n","# Let's compare the outputs\n","output_tf, output_pt = model_tf(input_tf), model_pt(**input_pt)\n","\n","# Models outputs 2 values (The value for each tokens, the pooled representation of the input sentence)\n","# Here we compare the output differences between PyTorch and TensorFlow.\n","for name in [\"last_hidden_state\", \"pooler_output\"]:\n","    print(\"{} differences: {:.5}\".format(name, (output_tf[name].numpy() - output_pt[name].numpy()).sum()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["last_hidden_state differences: 4.5233e-06\n","pooler_output differences: -8.0713e-06\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CQf_fpApSXao","pycharm":{"name":"#%% md\n"}},"source":["## Want it lighter? Faster? Let's talk distillation! \n","\n","One of the main concerns when using these Transformer based models is the computational power they require. All over this notebook we are using BERT model as it can be run on common machines but that's not the case for all of the models.\n","\n","For example, Google released a few months ago **T5** an Encoder/Decoder architecture based on Transformer and available in `transformers` with no more than 11 billions parameters. Microsoft also recently entered the game with **Turing-NLG** using 17 billions parameters. This kind of model requires tens of gigabytes to store the weights and a tremendous compute infrastructure to run such models which makes it impracticable for the common man !\n","\n","![transformers-parameters](https://github.com/huggingface/notebooks/blob/master/examples/images/model_parameters.png?raw=true)\n","\n","With the goal of making Transformer-based NLP accessible to everyone we @huggingface developed models that take advantage of a training process called **Distillation** which allows us to drastically reduce the resources needed to run such models with almost zero drop in performances.\n","\n","Going over the whole Distillation process is out of the scope of this notebook, but if you want more information on the subject you may refer to [this Medium article written by my colleague Victor SANH, author of DistilBERT paper](https://medium.com/huggingface/distilbert-8cf3380435b5), you might also want to directly have a look at the paper [(Sanh & al., 2019)](https://arxiv.org/abs/1910.01108)\n","\n","Of course, in `transformers` we have distilled some models and made them available directly in the library ! "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":270,"referenced_widgets":["5f9097f4567f4bbc857aa8c82e4db738","c98b128284f44ca1b01a73908fae5048","e79a9e3733d2401bb4cfe17385bd2c37","52ae5d5c0cb64879ba031160f56f9436","869ed6bc7c6b443fa8ddb5f46d35570a","011eb3aab41e4b949e955532d5743c6f","71beaec2fa9946c396f8d431a47ce653","38340fe884d54d3795aea136fb629197","a96481187e7d43338e6b57c1ef028704","ebe977fc97f6404db1419df62155d93d","ef3f0470bd4c4703a540aeff8b195376","e884e5db4a8348c9b2bbe768384ddb38","19540dc05f91476883bc2ef0fcba770c","81c23f3b17344c039feb6c672a613c8f","665edb82b1634ec49fd3f0975a9ec8f0","c8ac18c961b74ce890a32cf31f6b86ce"]},"id":"wfxMOXb-SXao","pycharm":{"is_executing":false},"outputId":"7b99973e-e65b-4a4b-a473-73ef61e41008"},"source":["from transformers import DistilBertModel\n","\n","bert_distil = DistilBertModel.from_pretrained('distilbert-base-cased')\n","input_pt = tokenizer(\n","    'This is a sample input to demonstrate performance of distiled models especially inference time', \n","    return_tensors=\"pt\"\n",")\n","\n","\n","%time _ = bert_distil(input_pt['input_ids'])\n","%time _ = model_pt(input_pt['input_ids'])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5f9097f4567f4bbc857aa8c82e4db738","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=411.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a96481187e7d43338e6b57c1ef028704","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=263273408.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"},{"output_type":"stream","text":["CPU times: user 57.2 ms, sys: 1 ms, total: 58.2 ms\n","Wall time: 66 ms\n","CPU times: user 108 ms, sys: 63 µs, total: 108 ms\n","Wall time: 108 ms\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7lSIc7FbSXao"},"source":["## Community provided models\n","\n","Last but not least, earlier in this notebook we introduced Hugging Face `transformers` as a repository for the NLP community to exchange pretrained models. We wanted to highlight this features and all the possibilities it offers for the end-user.\n","\n","To leverage community pretrained models, just provide the organisation name and name of the model to `from_pretrained` and it will do all the magic for you ! \n","\n","\n","We currently have more 50 models provided by the community and more are added every day, don't hesitate to give it a try !"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":390,"referenced_widgets":["be4f5c195a024264ac043e91225a2123","6e8820e81fb94f77875bdcdc2d6353eb","703cefb637164b2dabb4e1048d0bb1ce","cfa50509fbdc4a529049279185f177c7","1d01eedd955b4a9ba270a57cded26a75","cdfb36379ad34f4d99b4c5c82f2bd12f","0ca4396c1653446681667e3a4c7d9010","8ad1c98c6e424a9eacb14a9e41395231","d9e33ac6a1054bbbb6e65bf0a84d2a30","0f0f993d2c19441ba63c1a61d3e5d74a","80a1b97710e34d97950091831d0e1ff4","bda6cfd394a94033860cf9ec3d697449","a213b02300d2479dacaace70f505e542","9a9e37c19cd2452c82da0fcf65c0419f","b7f049fa0a7e42e9b50e092d5c55caa0","d76aa60247f94d32866a29d0c0df5f17","974925b377cc43a8b43ca188810b00c7","8edbdcc86f284db99b3f223a9faa478c","d5039e3e39c6414886c96bfb54511500","76bb044a039d46d2994b38883746b70d","953290850c8e42d78172812440c9b0a9","c33470fbcc5947a1ab79eeae5a3038f7","9942063cdf6a4a499526f41a1377f838","f48704508cd043d3bdca8babfa34d18d","42731d52f5414f1f8dfcd919946bc1a7","6c3d8f8451704c198653b42e9c196233","ac62e8db919e499b8b64642cf173efb8","4cbc41b07abd4fcdb45ea930d574ada6","280af4a878d940b683d541a7d29a5bbf","182ea22e1df54abbbaeb631b54298c57","93e38e0997b74de79e0b4e615a325588","36d2cccef5b44f8caf52c8fb19be7c6d"]},"id":"cxLYnadGSXao","pycharm":{"is_executing":false},"outputId":"efe12fd2-42f0-48b9-b1d8-0490459115b6"},"source":["# Let's load German BERT from the Bavarian State Library\n","de_bert = BertModel.from_pretrained(\"dbmdz/bert-base-german-cased\")\n","de_tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-german-cased\")\n","\n","de_input = de_tokenizer(\n","    \"Hugging Face ist eine französische Firma mit Sitz in New-York.\",\n","    return_tensors=\"pt\"\n",")\n","print(\"Tokens (int)      : {}\".format(de_input['input_ids'].tolist()[0]))\n","print(\"Tokens (str)      : {}\".format([de_tokenizer.convert_ids_to_tokens(s) for s in de_input['input_ids'].tolist()[0]]))\n","print(\"Tokens (attn_mask): {}\".format(de_input['attention_mask'].tolist()[0]))\n","print()\n","\n","outputs_de = de_bert(**de_input)\n","last_hidden_state_de = outputs_de.last_hidden_state\n","pooler_output_de = outputs_de.pooler_output\n","\n","print(\"Token wise output: {}, Pooled output: {}\".format(last_hidden_state_de.shape, pooler_output_de.shape))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"be4f5c195a024264ac043e91225a2123","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d9e33ac6a1054bbbb6e65bf0a84d2a30","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442252390.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at dbmdz/bert-base-german-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"974925b377cc43a8b43ca188810b00c7","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=239836.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"42731d52f5414f1f8dfcd919946bc1a7","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=59.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Tokens (int)      : [102, 12272, 9355, 5746, 30881, 215, 261, 5945, 4118, 212, 2414, 153, 1942, 232, 3532, 566, 103]\n","Tokens (str)      : ['[CLS]', 'Hug', '##ging', 'Fac', '##e', 'ist', 'eine', 'französische', 'Firma', 'mit', 'Sitz', 'in', 'New', '-', 'York', '.', '[SEP]']\n","Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","\n","Token wise output: torch.Size([1, 17, 768]), Pooled output: torch.Size([1, 768])\n"],"name":"stdout"}]}]}