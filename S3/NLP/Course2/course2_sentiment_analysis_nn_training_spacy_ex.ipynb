{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"course2_sentiment_analysis_nn_training_spacy_ex.ipynb","provenance":[{"file_id":"1ZLtADH99lJ7RoRXuBehXYdya0OnNrx7J","timestamp":1622911279688},{"file_id":"1UcqV7dVM8DE7kCjCTwiBtXaNBm1pkeVr","timestamp":1621959843296}],"authorship_tag":"ABX9TyNimLpBpz3Ec1df9nIcV73U"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"S_2Ay0Fp6Bbs"},"source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import pandas as pd\n","import numpy as np\n","import pickle\n","from tensorflow import keras\n","from tensorflow.keras.layers import Dense, Dropout"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RNx6uhxeWDAx","executionInfo":{"status":"ok","timestamp":1622909692159,"user_tz":-120,"elapsed":45013,"user":{"displayName":"Romain Benassi","photoUrl":"","userId":"07755091784567448642"}},"outputId":"5a243e47-62d8-42ff-edd8-3c1368609ec6"},"source":["import spacy\n","import spacy.cli\n","from scipy import spatial\n","spacy.cli.download(\"en_core_web_md\")\n","nlp = spacy.load('en_core_web_md')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_md')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"b2CIb2eM9BEx"},"source":["Definition of a plot function for training result visualization"]},{"cell_type":"code","metadata":{"id":"_C-ARKUf6Zs4"},"source":["def plot_results(history):\n","    hist_df = pd.DataFrame(history.history)\n","    hist_df.columns=[\"loss\", \"accuracy\", \"val_loss\", \"val_accuracy\"]\n","    hist_df.index = np.arange(1, len(hist_df)+1)\n","    \n","    fig, axs = plt.subplots(nrows=2, sharex=True, figsize=(16, 10))\n","    axs[0].plot(hist_df.val_accuracy, lw=3, label='Validation Accuracy')\n","    axs[0].plot(hist_df.accuracy, lw=3, label='Training Accuracy')\n","    axs[0].set_ylabel('Accuracy')\n","    axs[0].set_xlabel('Epoch')\n","    axs[0].grid()\n","    axs[0].legend(loc=0)\n","    axs[1].plot(hist_df.val_loss, lw=3, label='Validation Loss')\n","    axs[1].plot(hist_df.loss, lw=3, label='Training Loss')\n","    axs[1].set_ylabel('Loss')\n","    axs[1].set_xlabel('Epoch')\n","    axs[1].grid()\n","    axs[1].legend(loc=0)\n","    \n","    plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KwWZyV3V9eoA"},"source":["IMDB database loading"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RqPNY0mG9Gga","executionInfo":{"status":"ok","timestamp":1622909730431,"user_tz":-120,"elapsed":16544,"user":{"displayName":"Romain Benassi","photoUrl":"","userId":"07755091784567448642"}},"outputId":"74b2a4de-c24e-485a-b067-c3908915171b"},"source":["# Mounting the google drive to google colab in order to load the data files directly from it\n","from google.colab import drive\n","drive.mount('/content/drive')\n","imdb_df = pd.read_csv(\"/content/drive/MyDrive/EPITA_NLP/Course2/IMDB Dataset.csv\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"p0jhzfdl9Jh7"},"source":["# These data from IMDB correspond to movie review, with sentiment (postive/negative) labels\n","imdb_df.head"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HKWXcmfwaz0m"},"source":["len(imdb_df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AEGjlF9S9jJ9"},"source":["## Preprocessing of the data"]},{"cell_type":"markdown","metadata":{"id":"MvHI_pLN9s2L"},"source":["We get the IMDB dataset"]},{"cell_type":"markdown","metadata":{"id":"rFRZa_w7JHPB"},"source":["Here is the original code to transform the texts into 300-dimension embedding vectors with spaCy pretrained model.\n","However, in order to save most of the computation time needed, we just do the operation **on a small subsample** of the complete data. On the **next cells**, we load directly the **embedding already computed on all the data** to use it directly."]},{"cell_type":"markdown","metadata":{"id":"XURwCde9L9Ox"},"source":["Let's have a look to an example of review"]},{"cell_type":"code","metadata":{"id":"2kSI0CRhLqOf"},"source":["review_example = imdb_df[\"review\"][0]\n","review_example"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BZrxZWgqMEwT"},"source":["Let's see how to use spaCy to turn it into a 300-dimension **text** embedding vector"]},{"cell_type":"code","metadata":{"id":"3-UqcHdLL071"},"source":["text_embedding_example =  nlp(review_example).vector\n","text_embedding_example"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f46ih7ewMWEi"},"source":["text_embedding_example.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Egh1b2Sd-4sL"},"source":["As explained just before, we only compute the embedding on the 100 first element in order to avoid a too long computation time"]},{"cell_type":"code","metadata":{"id":"pLr7YiH96eB2"},"source":["size_data = 100#len(test_df)\n","list_embed = [float('nan')] * size_data\n","list_label = [float('nan')] * size_data\n","\n","compt = 0\n","for sentence in list(imdb_df.itertuples())[0:size_data]:\n","    text_embed = nlp(sentence.review).vector\n","    observed_sentiment = sentence.sentiment\n","    if  observed_sentiment==\"positive\":\n","      label = 1.0\n","    else:\n","      label = 0.0\n","    list_embed[compt] = np.asarray(text_embed, dtype =\"float32\").reshape(1,300)\n","    list_label[compt] = label\n","    compt += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bxfLlenY-MKh"},"source":["print(list_label)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e38nWWPY-bpG"},"source":["len(list_embed)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FVdNyQSLJxf-"},"source":["To save time processing, we download the **embeddings already computed on the complete dataset**. It may take some time anyway but it is still much less than doing the whole operation once again."]},{"cell_type":"code","metadata":{"id":"J52KgwiPIQrt"},"source":["df_imdb_embed_label = pd.read_pickle(\"/content/drive/MyDrive/EPITA_NLP/Course2/df_imdb_embed_label.pkl\")\n","list_embed = list(df_imdb_embed_label[\"embedding\"])\n","list_label = list(df_imdb_embed_label[\"label\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kGOyay4nAHZ6"},"source":["len(list_embed)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uxitmaEZA64u"},"source":["## Use of a neural network to perform sentiment analysis from the spaCy embeddings"]},{"cell_type":"markdown","metadata":{"id":"aXJvUiwrA-4i"},"source":["## Neural network model definition"]},{"cell_type":"markdown","metadata":{"id":"ARfgvSuyDpnl"},"source":["Build a neural network using keras sequential layers\n","\n","(you may have a look at https://keras.io/api/layers/)"]},{"cell_type":"code","metadata":{"id":"kobUaul57W0y"},"source":["# Question 1: Build a neural network using relevant layers, dimensions and activation function (the input layer is already defined to help you)\n","model = tf.keras.models.Sequential([\n","    Dense(300, activation=\"relu\"),\n","    #??????\n","    #??????\n","    #....\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7nZqZYSNC9Am"},"source":["We build the model and check that everything is fine"]},{"cell_type":"code","metadata":{"id":"sxoDBFOhCF7q"},"source":["model(list_embed[0])\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vXeRGfGzDCVo"},"source":["We compile the model, choosing the relevant loss function, optimizer and metrics"]},{"cell_type":"markdown","metadata":{"id":"IRvaXLmJE5dj"},"source":["(You may have a look at\n","https://keras.io/api/losses/\n","and\n","https://keras.io/api/optimizers/)"]},{"cell_type":"code","metadata":{"id":"rYIi8jps7aFa"},"source":["# Question 2: Choose a relevant loss fonction and optimizer for the training\n","loss_function = # ?????\n","optimizer = # ??????\n","\n","model.compile(loss=loss_function, optimizer=optimizer,\n","              metrics=[\"accuracy\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n2kleOxiDISr"},"source":["We train the model on the dataset"]},{"cell_type":"code","metadata":{"id":"8E35LLMttih3"},"source":["n_dim_embedding = 300\n","size_data = 10000\n","array_embed = np.asarray(list_embed[0:size_data]).reshape(size_data,n_dim_embedding)\n","array_label = np.asarray(list_label[0:size_data])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O6TTGWXb7eEn"},"source":["# Question 3: Choose relevant values for epochs, batch_size and validation_split\n","# (Start with small values for epochs in order to save some computation time)\n","epochs = # ?????\n","batch_size = # ?????\n","validation_split = # ?????\n","\n","history = model.fit(x = array_embed, y = array_label, epochs = epochs, batch_size= 1, validation_split=0.1)\n","#history = model.fit(x = np.asarray(list_embed), y = np.asarray(list_label), epochs = epochs, batch_size= batch_size, validation_split=validation_split)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h15OAnNqBdP4"},"source":["## Result visualization"]},{"cell_type":"code","metadata":{"id":"WXjErDMg7lTD"},"source":["plot_results(history)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hZdJhtwHFljl"},"source":["# Question 4: What can you tell about the results? Does it seem satisfying to you? Do you see any hint of an over-fitting? If yes, what kind of layers can you use into the Keras model in order to prevent this phenomenon?"],"execution_count":null,"outputs":[]}]}