{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"course2_sentiment_analysis_nn_training_ex.ipynb","provenance":[{"file_id":"1UcqV7dVM8DE7kCjCTwiBtXaNBm1pkeVr","timestamp":1622912798930}],"authorship_tag":"ABX9TyNTWkAU4sFCEYvQ0IAY4bQQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"S_2Ay0Fp6Bbs"},"source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import pandas as pd\n","import numpy as np\n","from tensorflow import keras\n","from tensorflow.keras.layers import Dense, Dropout"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b2CIb2eM9BEx"},"source":["Definition of a plot function for training result visualization"]},{"cell_type":"code","metadata":{"id":"_C-ARKUf6Zs4"},"source":["def plot_results(history):\n","    hist_df = pd.DataFrame(history.history)\n","    hist_df.columns=[\"loss\", \"accuracy\", \"val_loss\", \"val_accuracy\"]\n","    hist_df.index = np.arange(1, len(hist_df)+1)\n","    \n","    fig, axs = plt.subplots(nrows=2, sharex=True, figsize=(16, 10))\n","    axs[0].plot(hist_df.val_accuracy, lw=3, label='Validation Accuracy')\n","    axs[0].plot(hist_df.accuracy, lw=3, label='Training Accuracy')\n","    axs[0].set_ylabel('Accuracy')\n","    axs[0].set_xlabel('Epoch')\n","    axs[0].grid()\n","    axs[0].legend(loc=0)\n","    axs[1].plot(hist_df.val_loss, lw=3, label='Validation Loss')\n","    axs[1].plot(hist_df.loss, lw=3, label='Training Loss')\n","    axs[1].set_ylabel('Loss')\n","    axs[1].set_xlabel('Epoch')\n","    axs[1].grid()\n","    axs[1].legend(loc=0)\n","    \n","    plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AEGjlF9S9jJ9"},"source":["## Preprocessing of the data"]},{"cell_type":"markdown","metadata":{"id":"MvHI_pLN9s2L"},"source":["We get the IMDB dataset directly from the tensorflow_datasets API and we do the usual preprocessing before feeding a neural network"]},{"cell_type":"code","metadata":{"id":"pLr7YiH96eB2"},"source":["import tensorflow_datasets as tfds\n","\n","datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n","\n","train_size = info.splits[\"train\"].num_examples\n","batch_size = 32\n","\n","train_set = datasets[\"train\"].shuffle(10000).repeat().batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","test_size = info.splits[\"test\"].num_examples\n","test_set = datasets[\"test\"].repeat().batch(batch_size).prefetch(tf.data.AUTOTUNE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uxitmaEZA64u"},"source":["## Use of a pretrained embedding"]},{"cell_type":"markdown","metadata":{"id":"PjjSBh_H-LRA"},"source":["We use of pretrained embedding directly from tensorflow_hub"]},{"cell_type":"code","metadata":{"id":"zU7z0p_V7O7A"},"source":["embed = hub.load(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ab7EQzFmAXmu"},"source":["We test on two (famous) lines and check the shapes of the embedding results"]},{"cell_type":"code","metadata":{"id":"YQR2XvIk7S03"},"source":["embeddings = embed([\"A thing of beauty is a joy forever\", \"If by dull rhymes our English must be chain'd\"])\n","print(embeddings)\n","print(embeddings.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aXJvUiwrA-4i"},"source":["## Neural network model definition"]},{"cell_type":"markdown","metadata":{"id":"5t7Et6bMGoz4"},"source":["Build a neural network using keras sequential layers\n","\n","(you may have a look at https://keras.io/api/layers/)"]},{"cell_type":"code","metadata":{"id":"kobUaul57W0y"},"source":["# Question 1: Build a neural network using relevant layers, dimensions and activation function (the input layer is already defined to help you)\n","model = tf.keras.models.Sequential([\n","    hub.KerasLayer(embed,\n","                   dtype=tf.string, input_shape=[], output_shape=[50]),\n","    #??????\n","    #??????\n","    #....\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jUFjZK83Gs99"},"source":["We check that everything is fine with the model as we defined it"]},{"cell_type":"code","metadata":{"id":"u7eZA1saGuXa"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kg70hqlPJbaU"},"source":["We compile the model, choosing the relevant loss function, optimizer and metrics"]},{"cell_type":"markdown","metadata":{"id":"IkIzJDn9G0Av"},"source":["(You may have a look at\n","https://keras.io/api/losses/\n","and\n","https://keras.io/api/optimizers/)"]},{"cell_type":"code","metadata":{"id":"rYIi8jps7aFa"},"source":["# Question 2: Choose a relevant loss fonction and optimizer for the training\n","loss_function = # ?????\n","optimizer = # ??????\n","\n","model.compile(loss=loss_function, optimizer=optimizer,\n","              metrics=[\"accuracy\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"siQfzR5oHHni"},"source":["We train the model on the dataset"]},{"cell_type":"code","metadata":{"id":"O6TTGWXb7eEn"},"source":["# Question 3: Choose relevant values for epochs\n","# (Start with small values for epochs in order to save some computation time)\n","epochs = # ?????\n","\n","history = model.fit(train_set, steps_per_epoch=train_size // batch_size, epochs=epochs, validation_data=test_set, validation_steps=test_size // batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h15OAnNqBdP4"},"source":["## Result visualization"]},{"cell_type":"code","metadata":{"id":"WXjErDMg7lTD"},"source":["plot_results(history)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wgshsvFRKC4_"},"source":["# Question 4: What can you tell about the results? Does it seem satisfying to you? Do you see any hint of an over-fitting? If yes, what kind of layers can you use into the Keras model in order to prevent this phenomenon?"],"execution_count":null,"outputs":[]}]}