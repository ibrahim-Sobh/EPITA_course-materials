{"cells":[{"cell_type":"markdown","metadata":{"id":"W9cxlIq3CtOa"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"iWVBvSIx2yiT"},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import pandas as pd\n","import numpy as np\n","from tensorflow import keras\n","from tensorflow.keras.layers import Dense, Dropout, Bidirectional, GRU, LSTM, Embedding"]},{"cell_type":"markdown","metadata":{"id":"y5RciN0LIzOb"},"source":["Definition of a plot function for training result visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"cXWhKMLm3weE"},"outputs":[],"source":["def plot_results(history):\n","    hist_df = pd.DataFrame(history.history)\n","    hist_df.columns=[\"loss\", \"accuracy\", \"val_loss\", \"val_accuracy\"]\n","    hist_df.index = np.arange(1, len(hist_df)+1)\n","    \n","    fig, axs = plt.subplots(nrows=2, sharex=True, figsize=(16, 10))\n","    axs[0].plot(hist_df.val_accuracy, lw=3, label='Validation Accuracy')\n","    axs[0].plot(hist_df.accuracy, lw=3, label='Training Accuracy')\n","    axs[0].set_ylabel('Accuracy')\n","    axs[0].set_xlabel('Epoch')\n","    axs[0].grid()\n","    axs[0].legend(loc=0)\n","    axs[1].plot(hist_df.val_loss, lw=3, label='Validation Loss')\n","    axs[1].plot(hist_df.loss, lw=3, label='Training Loss')\n","    axs[1].set_ylabel('Loss')\n","    axs[1].set_xlabel('Epoch')\n","    axs[1].grid()\n","    axs[1].legend(loc=0)\n","    \n","    plt.show();"]},{"cell_type":"markdown","metadata":{"id":"dOY1622pI28O"},"source":["## Preprocessing of the data"]},{"cell_type":"markdown","metadata":{"id":"QNrVgIYtI4A5"},"source":["We get the IMDB dataset directly from the tensorflow_datasets API"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"IyA1_yxX3yXj"},"outputs":[],"source":["import tensorflow_datasets as tfds\n","\n","datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n"]},{"cell_type":"markdown","metadata":{"id":"18dkwf-KJFZL"},"source":["We build train and test sets, getting both the string input data as well as the labels"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-ahFZyLx3_Ti"},"outputs":[],"source":["train_data, test_data = datasets['train'], datasets['test']\n","\n","training_sentences = []\n","training_labels = []\n","\n","testing_sentences = []\n","testing_labels = []\n","\n","for s,l in train_data:\n","  training_sentences.append(str(s.numpy()))\n","  training_labels.append(l.numpy())\n","  \n","for s,l in test_data:\n","  testing_sentences.append(str(s.numpy()))\n","  testing_labels.append(l.numpy())\n","  \n","training_labels_final = np.array(training_labels)\n","testing_labels_final = np.array(testing_labels)"]},{"cell_type":"markdown","metadata":{"id":"k2S91GJEKCzs"},"source":["We can check some values of the data to have a good understanding of it"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"dBf3h41GJ1PT"},"outputs":[],"source":["training_sentences[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"inMgJKirJ-NZ"},"outputs":[],"source":["training_labels[0]"]},{"cell_type":"markdown","metadata":{"id":"aex5LOT7KWZf"},"source":["We create, from the text reviews, padding sequences of token values"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"xWR3QRqB4VL2"},"outputs":[],"source":["vocab_size = 10000\n","embedding_dim = 16\n","max_length = 120\n","trunc_type='post'\n","oov_tok = \"<OOV>\"\n","\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n","tokenizer.fit_on_texts(training_sentences)\n","word_index = tokenizer.word_index\n","sequences = tokenizer.texts_to_sequences(training_sentences)\n","padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n","\n","testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n","testing_padded = pad_sequences(testing_sequences,maxlen=max_length)"]},{"cell_type":"markdown","metadata":{"id":"KuoEH8acMeXz"},"source":["Have a look to some values in order to understand how we go from text sentences to sequences of tokens and, finally, to zero-padded sequences"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"CNUtJ6-_K_3I","outputId":"b26af5c0-adaa-4171-dfdd-faa1c02a433d"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'b\"I really tried, but this movie just didn\\'t work for me. The action scenes were dull, the acting was surprisingly poor, and some of these characters were TOO stereotypical to even be funny. Pam Grier tries, but when you have nothing to work with, even her considerable talent cannot prevent a disaster. Even by the standards of this weak genre, this film is pretty bad.\"'"]},"execution_count":0,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["testing_sentences[20]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fWQJiSY5LFHR","outputId":"3ed87fd3-4e31-4cff-bb41-68cdffe76fcb"},"outputs":[{"name":"stdout","output_type":"stream","text":["[59, 11, 68, 812, 19, 12, 18, 43, 330, 157, 16, 73, 2, 204, 140, 72, 767, 2, 116, 14, 1255, 344, 3, 50, 5, 135, 106, 72, 100, 2794, 6, 62, 29, 162, 8844, 1, 506, 19, 55, 23, 28, 163, 6, 157, 17, 62, 40, 4452, 682, 577, 3673, 4, 1712, 62, 33, 2, 1574, 5, 12, 830, 517, 12, 20, 7, 184, 80]\n","length of the sequence: 66\n"]}],"source":["print(testing_sequences[20])\n","print(f\"length of the sequence: {len(testing_sequences[20])}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UmuJf8hpLObw"},"outputs":[],"source":["print(testing_padded[20])\n","print(f\"length of the sequence: {len(testing_padded[20])}\")"]},{"cell_type":"markdown","metadata":{"id":"yHD53tS8PIBI"},"source":["## Neural network model with LSTM"]},{"cell_type":"markdown","metadata":{"id":"vk945wQDPLIa"},"source":["Build a neural network using at least one LSTM layer\n","\n","(you may have a look at https://keras.io/api/layers/)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2hQqCPmP4knG"},"outputs":[],"source":["# Question 1: define a neural network model using at least one LSTM layer\n","# Hint1: we give you the first Embedding layer to help you, you can feed it directly with the padded sequences\n","# Hint2: do not restreint yourself to the LSTM layers, you can use as well Dense, Bidirectional, GRU, Dropout layers\n","\n","model = tf.keras.Sequential([\n","    Embedding(vocab_size, embedding_dim, input_length=max_length),\n","    # ??????\n","])\n","\n","# Question 2: define a relevant loss function and optimizer\n","\n","# loss_function = ??????\n","# optimizer = ??????\n","model.compile(loss=loss_function, optimizer=optimizer,metrics=['accuracy'])\n","model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"zX68pSZ4PdHj"},"outputs":[],"source":["# Try first with a reasonnable number of epochs, before increasing the number\n","num_epochs = 10\n","history = model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hysDx88jPgEs"},"outputs":[],"source":["plot_results(history)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNCGNTJSznoeBYS9aU6toD0","name":"course3_sentiment_analysis_LSTM_ex.ipynb","provenance":[{"file_id":"13mRhMU3nVA5C5T52hKxxL_6NK0YM24DM","timestamp":1623883789462},{"file_id":"1ACajf-NYa7betfbi8NbXNVGfUiWhNpBD","timestamp":1623883112365}]},"kernelspec":{"display_name":"Python 3.9.12","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.12"},"vscode":{"interpreter":{"hash":"3592ed04a6035a70cc4700cfa7dd5dceeef194c6f9ccd0164c760f6184dadd52"}}},"nbformat":4,"nbformat_minor":0}
