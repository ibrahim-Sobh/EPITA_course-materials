{"cells":[{"cell_type":"markdown","metadata":{"id":"-urn6K-R4zoQ"},"source":["This notebook serves as a training for the basics text sequence preprocessing to do in order to train RNN or LSTM networks"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1623881781429,"user":{"displayName":"Romain Benassi","photoUrl":"","userId":"07755091784567448642"},"user_tz":-120},"id":"2aVhL2fkrMH6"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"37ZZ8UCdrd-u"},"source":["## Preprocessing of the data"]},{"cell_type":"markdown","metadata":{"id":"Liw-_SS2roy4"},"source":["We get the IMDB dataset directly from the tensorflow_datasets API"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sUcOr9j9rn2T"},"outputs":[],"source":["import tensorflow_datasets as tfds\n","\n","datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)"]},{"cell_type":"markdown","metadata":{"id":"57N_uRFcr36c"},"source":["We reformate the training data to make easier the preprocessing wanted"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":6620,"status":"ok","timestamp":1623881847105,"user":{"displayName":"Romain Benassi","photoUrl":"","userId":"07755091784567448642"},"user_tz":-120},"id":"73mPIRanr8Vz"},"outputs":[],"source":["train_data = datasets['train']\n","\n","training_sentences = []\n","training_labels = []\n","\n","for s,l in train_data:\n","  training_sentences.append(str(s.numpy()))\n","  training_labels.append(l.numpy())\n","  \n","training_labels_final = np.array(training_labels)"]},{"cell_type":"markdown","metadata":{"id":"NRUA44hC2o9o"},"source":["## Tokenization"]},{"cell_type":"markdown","metadata":{"id":"_kMGeUwWs2Ge"},"source":["Use of the Tokenizer object from Keras to tokenize the sequence.\n","\n","(You can have a look at the doc page https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":7703,"status":"ok","timestamp":1623881854804,"user":{"displayName":"Romain Benassi","photoUrl":"","userId":"07755091784567448642"},"user_tz":-120},"id":"k7dvn-SEs0f1"},"outputs":[],"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","tokenizer = Tokenizer()\n","tokenized_sequences=tokenizer.fit_on_texts(training_sentences)\n","# Question 1: use the \"tokenizer\" object to transform the original texts into tokenized sequences \n","\n","# tokenized_sequences = ??????\n","tokenizer.fit_on_texts(training_sentences)\n","test=tokenizer.texts_to_sequences(training_sentences)\n"]},{"cell_type":"markdown","metadata":{"id":"L719Mlcp2h9B"},"source":["We can check if we really get a sequence of tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9_I7zq3iu6Ib"},"outputs":[],"source":["print(tokenized_sequences[0])"]},{"cell_type":"markdown","metadata":{"id":"BBj9JOiN3Mg5"},"source":["We can have a look at the explicit correspondance between words and token numbers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EZ6PyspLyNKt"},"outputs":[],"source":["tokenizer.word_index"]},{"cell_type":"markdown","metadata":{"id":"tZi1bpPY2ryU"},"source":["## Padding"]},{"cell_type":"markdown","metadata":{"id":"VqJE7JszvM1z"},"source":["Use of the pad_sequences function to pad the tokenized sequences created just before.\n","\n","(See https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":631,"status":"ok","timestamp":1623881855748,"user":{"displayName":"Romain Benassi","photoUrl":"","userId":"07755091784567448642"},"user_tz":-120},"id":"g1pwNBZ5u8zi"},"outputs":[],"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Question 2: use the \"pad_sequences\" function to transform the tokenized sequences into padded sequences\n","\n","#max_length = ?????? # choose a length value the padded sequences\n","#padded_sequences = ??????\n"]},{"cell_type":"markdown","metadata":{"id":"JzfHT5E_3Vs9"},"source":["We can check if we really get a padded sequence of tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kSs0QBMnvrcM"},"outputs":[],"source":["print(padded_sequences[0])"]},{"cell_type":"markdown","metadata":{"id":"ffeV-PSP3doQ"},"source":["A quick comparison between the padded and the original sequences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3w_DiAkdv33i"},"outputs":[],"source":["padded_sequence_length = len(padded_sequences[0])\n","original_sequence_length = len(tokenized_sequences[0])\n","\n","print(f\"The length of the padded sequence is {padded_sequence_length} whereas it used to be {original_sequence_length} before padding\\n\")\n","\n","if padded_sequence_length>original_sequence_length:\n","  print(\"You should observed zero-padding on the new sequence\")\n","else:\n","    print(\"You should observed a troncation of the original sequence\")\n"]},{"cell_type":"markdown","metadata":{"id":"aHoYdu2m2u0X"},"source":["## One-hot encoding"]},{"cell_type":"markdown","metadata":{"id":"ed1m-5dv9pJC"},"source":["Use of the to_categorical function to convert each token of a sequence into its one-hot encoded version\n","\n","(See https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical)"]},{"cell_type":"markdown","metadata":{"id":"ndJVlXRB2y2v"},"source":["We try only on the first sequence, in order to prevent memory crashing"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1623881855750,"user":{"displayName":"Romain Benassi","photoUrl":"","userId":"07755091784567448642"},"user_tz":-120},"id":"sTUFZThpxEsF"},"outputs":[],"source":["from tensorflow.keras.utils import to_categorical\n","\n","first_padded_sequence = padded_sequences[0]\n","\n","# Question 3: use the \"to_categorical\" function to transform the first padded sequence (and it only, to prevent memory crash) into a one-hot encoded sequence\n","\n","#one_hot_encoded_first_sequence = ??????"]},{"cell_type":"markdown","metadata":{"id":"3rAIad2K3mrc"},"source":["We can check if we really get a sequence of one-hot-encoded tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CVixwLysxXES"},"outputs":[],"source":["print(one_hot_encoded_first_sequence)"]},{"cell_type":"markdown","metadata":{"id":"5ZviXYKO4DZZ"},"source":["As we saw in the class, most of the time we will not need to explictly compute the one-hot encoded sequence vectors"]},{"cell_type":"markdown","metadata":{"id":"wUCI0Ozw4TB1"},"source":["## Reverse processing: from the padded sequences to the original text sequences"]},{"cell_type":"markdown","metadata":{"id":"G1UthLr74jJP"},"source":["We define a function to get the original sequences back from the padded ones"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iUQxScMXx3BC"},"outputs":[],"source":["# Question 4: Define a function to get the original sequences back from the padded ones\n","\n","# ??????"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyM3gW8BWolvTHES6u1yCM5k","name":"course3_text_sequence_preprocessing_ex.ipynb","provenance":[{"file_id":"1zqvtwXUXfZ9i1K32X27jMlxKp77adwMU","timestamp":1623881753712}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
