{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> LAB 2: Linear Classification: Logistic Regression<br> <small>RÃ©da DEHAK<br> December 9th, 2020</small> </center>\n",
    "\n",
    "The goal of this lab is :\n",
    "    - Test the logistic regression on classification problems\n",
    "    - Evaluate performances statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Logistic Regression \n",
    "### Import Data\n",
    "\n",
    "We will use the Wine dataset from UCI. These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of thirteen constituents found in each of the three types of wines.\n",
    "\n",
    "# Loading and Plotting Data\n",
    " \n",
    "First, we will use only two features from the data set: alcohol and ash (We can plot the solution in 2D space). The labels are supplied as an array of data with values from 1 to 3, but at first, we want a simple binary regression problem with a yes or no answer.  \n",
    "\n",
    "We filter the data set, reducing it to only include wines with labels 1 or 2.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "data = pd.read_csv('data.txt')\n",
    "\n",
    "reduced = data[data['class'] <= 2]\n",
    "X = reduced[['alcohol', 'ash']].to_numpy()\n",
    "y = label_binarize(reduced['class'].to_numpy(), classes=[1, 2])[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size = 0.25, random_state = 42)\n",
    "print('train:', len(Xtrain), 'test:', len(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKERS = ['+', 'x', '.']\n",
    "COLORS = ['red', 'green', 'blue']\n",
    "\n",
    "def plot_points(xy, labels):\n",
    "    \n",
    "    for i, label in enumerate(set(labels)):\n",
    "        points = np.array([xy[j,:] for j in range(len(xy)) if labels[j] == label])\n",
    "        marker = MARKERS[i % len(MARKERS)]\n",
    "        color = COLORS[i % len(COLORS)]\n",
    "        plt.scatter(points[:,0], points[:,1], marker=marker, color=color)\n",
    "\n",
    "plot_points(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we can plot line that could divide the two colored points with a small amount of error.\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "To implement logistic regression, we need to define the cost function $J(\\theta)$, and compute the partial derivatives of $J(\\theta)$. As we have seen previously:\n",
    "\n",
    "$$\n",
    "J(\\theta) =-\\frac{1}{N}\\sum_{i=1}^{N}y^{i}\\log(f_\\theta(x^{i}))+(1-y^{i})\\log(1-f_\\theta(x^{i}))\n",
    "$$\n",
    "\n",
    "where $f_\\theta(x)$ is the logistic function\n",
    "\n",
    "$$\n",
    "f_\\theta(x) = \\frac{1}{1 + e^{-\\theta^Tx}}\n",
    "$$\n",
    "\n",
    "- Compute the partiel derivatives of $J(\\theta)$ and write the two functions:\n",
    "    - cost(theta, X, y) which compute the value of $J(\\theta)$\n",
    "    - gradient(theta, X, y) which compute the value of the gradient of $J(\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(theta, X):\n",
    "    ...\n",
    "\n",
    "def cost(theta, X, y):\n",
    "    ...\n",
    "\n",
    "def gradient(theta, X, y):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the function scipy.optimize.fmin_tnc which performs a gradient descent algorithm, write a function Train(x, y) which compute $\\theta$ that minimize $J(\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_tnc  \n",
    "\n",
    "def train(X, y):\n",
    "    X = np.insert(X, 0, np.ones(len(X)), axis=1)\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    result = fmin_tnc(...)\n",
    "    \n",
    "    return result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- compute the value of the best $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot the boundary and checks that it is linear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def predict(theta, X):\n",
    "    X = np.insert(X, 0, np.ones(len(X)), axis=1)\n",
    "    return (sigmoid(theta, X) >= 0.5).astype(int)\n",
    "\n",
    "def plot_boundary(X, pred):\n",
    "    \n",
    "    x_min, x_max = X[:,0].min() - .1, X[:,0].max() + .1\n",
    "    y_min, y_max = X[:,1].min() - .1, X[:,1].max() + .1\n",
    "    \n",
    "    xs, ys = np.meshgrid(\n",
    "        np.linspace(x_min, x_max, 200),\n",
    "        np.linspace(y_min, y_max, 200)\n",
    "    )\n",
    "\n",
    "    xys = np.column_stack([xs.ravel(), ys.ravel()])\n",
    "    zs = pred(xys).reshape(xs.shape)\n",
    "    plt.contour(xs, ys, zs, colors='black')\n",
    "  \n",
    "plot_boundary(Xtrain, lambda x: predict(W, x))\n",
    "plot_points(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using sklearn.metrics, compute the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute all performance metrics that has been defined during the lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot the ROC curve of your classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot the Precision-Recall curve of your classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How can we obtain a quadratic boundary? check it and plot the boundary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def transform(x):\n",
    "    #return PolynomialFeatures(2).fit_transform(x)\n",
    "    return np.vstack((...))\n",
    "\n",
    "W = train(transform(Xtrain), ytrain)\n",
    "print(W)\n",
    "plot_points(Xtrain, ytrain)\n",
    "plot_boundary(Xtrain, lambda x: ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute all performance metrics of this new classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Logistic Regression\n",
    "\n",
    "The next step is something more interesting: we use a similar set of two features from the data set (this time alcohol and flavanoids), but with all three labels instead of two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['alcohol', 'flavanoids']].to_numpy()\n",
    "y = data[['class']].to_numpy()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size = 0.25, random_state = 42)\n",
    "ytrain = label_binarize(ytrain, classes = [1, 2, 3])\n",
    "plot_points(Xtrain, ytrain.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plotted data points again suggest some obvious linear boundaries between the three classes.\n",
    "\n",
    "We can solve this problem as three one-vs-all problems, and re-use all the previous code. In this part, we will try another solution inspired from softmax function known as softmax regression (See C.Bishop, \"Pattern Recognition and Machine Learning\", 2006, Springer).\n",
    "\n",
    "$$\n",
    "SoftMax_\\Theta(x, k) = \\frac{e^{\\theta_k^Tx}}{\\sum\\limits_{c=1}^K e^{\\theta_c^Tx}}\n",
    "$$\n",
    "\n",
    "The cost function is defined as follows:\n",
    "\n",
    "$$\n",
    "J(\\Theta) =-\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{k=1}^3\\left[y_k^{i}\\log\\left(SoftMax_\\Theta\\left(x^{i}, k\\right)\\right)\\right]\n",
    "$$\n",
    "\n",
    "- Propose a solution using the SoftMax function and test it with linear and quadratic separator? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(w, x):\n",
    "    ...\n",
    "\n",
    "def cost(w, x, y):\n",
    "    ... \n",
    "\n",
    "def grad(w, x, y):\n",
    "    ... \n",
    "\n",
    "def train(X, y):\n",
    "    X = np.insert(X, 0, np.ones(len(X)), axis=1)\n",
    "    theta = np.zeros((X.shape[1], y.shape[1]))\n",
    "    result = fmin_tnc(...)\n",
    "    \n",
    "    return result[0].reshape((X.shape[1], -1))\n",
    "\n",
    "W1 = train(...)\n",
    "print(W1)\n",
    "\n",
    "W2 = train(...)\n",
    "print(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_multi(x, w):\n",
    "    ...\n",
    "\n",
    "plot_points(Xtrain, ytrain.argmax(axis=1))\n",
    "plot_boundary(Xtrain, lambda x: predict_multi(x, W1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_points(Xtrain, ytrain.argmax(axis=1))\n",
    "plot_boundary(Xtrain, lambda x: ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute the accuracy, the precision and the recall of these classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "Next, we want to include all the features from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('class', 1).as_matrix()\n",
    "y = data.as_matrix(columns=['class'])\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size = 0.25, random_state = 42)\n",
    "ytrain = label_binarize(ytrain, classes = [1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are now significantly increasing the number of features, we apply regularisation  as part of new cost and gradient functions.  As we have seen with linear regression, regularization prevents overfitting, a situation where a large number of features allows the classifier to fit the training set *too* exactly, meaning that it fails to generalize well and perform accurately on data it hasn't yet seen.\n",
    "\n",
    "To avoid this problem, we add an additional term to the cost function\n",
    "\n",
    "$$\n",
    "J(\\Theta) =-\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{k=1}^3\\left[y_k^{i}\\log\\left(SoftMax_\\Theta\\left(x^{i}, k\\right)\\right)\\right] + \\frac{\\lambda}{2}\\left(\\left\\|\\theta_1\\right\\|_2^2+\\left\\|\\theta_2\\right\\|_2^2+\\left\\|\\theta_3\\right\\|_2^2\\right)\n",
    "$$\n",
    "\n",
    "- Compute the partiel derivatives of $J(\\theta)$ and define the update formula of the gradient descent algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a function that minimize $J(\\theta)$ and test it on the WINE dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compare with non regularized version?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py37SysTf15",
   "language": "python",
   "name": "py37systf15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
