{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> LAB 03: Backpropagation in Multilayer Neural Networks<br> <small>RÃ©da DEHAK<br> 06 January 2021</small> </center>\n",
    "\n",
    "The goal of this lab is :\n",
    "\n",
    "    - Understand neural networks and their layered architectures,\n",
    "    - Understand and implement backpropagation in `Numpy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Simple Case 2D problem \n",
    "### Import Data\n",
    "\n",
    "We will use the Wine dataset from UCI. These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of thirteen constituents found in each of the three types of wines.\n",
    "\n",
    "# Loading and Plotting Data\n",
    " \n",
    "First, we will use only two features from the data set: alcohol and ash (We can plot the solution in 2D space). The labels are supplied as an array of data with values from 1 to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('data.txt')\n",
    "\n",
    "X = data[['alcohol', 'flavanoids']].to_numpy()\n",
    "y = data[['class']].to_numpy().flatten() - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 133 test: 45\n"
     ]
    }
   ],
   "source": [
    "# split data into train and validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=None)\n",
    "print('train:', len(X_train), 'test:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "MARKERS = ['+', 'x', '.']\n",
    "COLORS = ['red', 'green', 'blue']\n",
    "\n",
    "def plot_points(xy, labels):\n",
    "    \n",
    "    for i, label in enumerate(set(labels)):\n",
    "        points = np.array([xy[j,:] for j in range(len(xy)) if labels[j] == label])\n",
    "        marker = MARKERS[i % len(MARKERS)]\n",
    "        color = COLORS[i % len(COLORS)]\n",
    "        plt.scatter(points[:,0], points[:,1], marker=marker, color=color)\n",
    "\n",
    "plot_points(X_train, y_train.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "- Normalization\n",
    "- Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.94210526  2.04511278]\n",
      "[0.79546936 0.97288627]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Data Normalization: mean = 0; standard deviation = 1.0\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(scaler.mean_)\n",
    "print(scaler.scale_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "y_train.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### One-hot encoding for class label data\n",
    "\n",
    "First let's define a helper function to compute the one hot encoding of an integer array for a fixed number of classes (similar to keras' `to_categorical`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y, n_classes):\n",
    "    return np.eye(n_classes)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "to_categorical(y=3, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(y=3, n_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(y=[0, 4, 9, 1], num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(y=[0, 4, 9, 1], n_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The softmax function\n",
    "\n",
    "Now let's implement the softmax vector function:\n",
    "\n",
    "$$\n",
    "softmax(\\mathbf{x}) = \\frac{1}{\\sum_{i=1}^{n}{e^{x_i}}}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "  e^{x_1}\\\\\\\\\n",
    "  e^{x_2}\\\\\\\\\n",
    "  \\vdots\\\\\\\\\n",
    "  e^{x_n}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    t=np.exp(X)\n",
    "    return t / np.sum(t,axis=-1,keepdims=True) # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that this works one vector at a time (and check that the components sum to one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.99662391e-01 3.35349373e-04 2.25956630e-06]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(softmax([10, 2, -3]))\n",
    "print(np.sum(softmax([10, 2, -3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a naive implementation of softmax might not be able process a batch of activations in a single call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.99662391e-01 3.35349373e-04 2.25956630e-06]\n",
      " [2.47262316e-03 9.97527377e-01 1.38536042e-11]]\n",
      "[1. 1.]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[10, 2, -3],\n",
    "              [-1, 5, -20]])\n",
    "print(softmax(X))\n",
    "print(np.sum(softmax(X), axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that given the true one-hot encoded class `Y_true` and some predicted probabilities `Y_pred` returns the negative log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003350108584163783\n"
     ]
    }
   ],
   "source": [
    "eps =1E-8\n",
    "def nllk(Y_true, Y_pred):\n",
    "    Y_true = np.asarray(Y_true)\n",
    "    Y_pred = np.asarray(Y_pred)\n",
    "    llk =-1* np.sum(Y_true*np.log(Y_pred + eps))/Y_true.shape[0]\n",
    "    return llk\n",
    "\n",
    "\n",
    "# Make sure that it works for a simple sample at a time\n",
    "print(nllk([1, 0, 0], [.99, 0.01, 0]))  #gives you the most possibilty to this prediction ( very good one )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the nll of a very confident yet bad prediction is a much higher positive number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5350563953295306\n"
     ]
    }
   ],
   "source": [
    "print(nllk([1, 0, 0], [0.01, 0.01, .98])) #this prediciton is bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the `sigmoid` and its element-wise derivative `dsigmoid` functions:\n",
    "\n",
    "$$\n",
    "sigmoid(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "dsigmoid(x) = sigmoid(x) \\cdot (1 - sigmoid(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    S =\n",
    "    return X\n",
    "\n",
    "\n",
    "def dsigmoid(X):\n",
    "    # TODO\n",
    "    return X\n",
    "\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "plt.plot(x, sigmoid(x), label='sigmoid')\n",
    "plt.plot(x, dsigmoid(x), label='dsigmoid')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement `forward` and `forward_keep_all` functions for a model with a hidden layer with a sigmoid activation function:\n",
    "  - $\\mathbf{h} = sigmoid\\left(\\mathbf{W}_h^T \\hat{\\mathbf{x}}\\right)$\n",
    "  - $\\mathbf{y} = softmax\\left(\\mathbf{W}_o^T \\hat{\\mathbf{h}}\\right)$\n",
    "\n",
    "- Notes: \n",
    "  - `forward_keep_activations` is similar to forward, but also returns hidden activations and pre activations;\n",
    "\n",
    "- Implement the grad function to compute all gradients; check that the gradients are well defined;\n",
    "\n",
    "- Implement the `train` and `loss` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-8\n",
    "\n",
    "\n",
    "class NeuralNet():\n",
    "    \"\"\"MLP with 1 hidden layer with a sigmoid activation\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # TODO\n",
    "        self.W_h = None\n",
    "        self.W_o = None\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # TODO\n",
    "        if len(X.shape) == 1:\n",
    "            return np.random.uniform(size=self.output_size,\n",
    "                                     high=1.0-EPSILON, low=EPSILON)\n",
    "        else:\n",
    "            return np.random.uniform(size=(X.shape[0], self.output_size),\n",
    "                                     high=1.0-EPSILON, low=EPSILON)\n",
    "    \n",
    "    def forward_keep_activations(self, X):\n",
    "        # TODO\n",
    "        z_h = 0.\n",
    "        h = 0.\n",
    "        y = np.random.uniform(size=self.output_size,\n",
    "                              high=1.0-EPSILON, low=EPSILON)\n",
    "        return y, h, z_h\n",
    "    \n",
    "    def loss(self, X, y):\n",
    "        # TODO\n",
    "        return 42.\n",
    "\n",
    "    def grad_loss(self, x, y_true):\n",
    "        # TODO\n",
    "        return {\"dEdW_h\": 0., \"dEdW_o\": 0.}\n",
    "\n",
    "    \n",
    "    def train(self, x, y, learning_rate, l2 = 0.):\n",
    "        # One step of Backpropagation on x\n",
    "        # l2 is the L2 regularization coefficient \n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(X))\n",
    "        else:\n",
    "            return np.argmax(self.forward(X), axis=1)\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        y_preds = np.argmax(self.forward(X), axis=1)\n",
    "        return np.mean(y_preds == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the class NeuralNet to train a classifier on Wine Dataset, try different values for n_hidden (1,2,3 and 5), different Learning rate (0.1, 1, .5 and .01) and different l2 (0, 0.001, 0.005, 0.01) see the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 1\n",
    "n_features = 2\n",
    "n_classes = 3\n",
    "model = NeuralNet(n_features, n_hidden, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.loss(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.accuracy(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, accuracies, accuracies_test = [], [], []\n",
    "losses.append(model.loss(X_train, y_train))\n",
    "accuracies.append(model.accuracy(X_train, y_train))\n",
    "accuracies_test.append(model.accuracy(X_test, y_test))\n",
    "\n",
    "print(\"Random init: train loss: %0.5f, train acc: %0.3f, test acc: %0.3f\"\n",
    "      % (losses[-1], accuracies[-1], accuracies_test[-1]))\n",
    "\n",
    "for epoch in range(15):\n",
    "    \n",
    "    for i in np.arange(X_train.shape[0]):\n",
    "        model.train(x[i, :], y[i], .1)\n",
    "\n",
    "    losses.append(model.loss(X_train, y_train))\n",
    "    accuracies.append(model.accuracy(X_train, y_train))\n",
    "    accuracies_test.append(model.accuracy(X_test, y_test))\n",
    "    print(\"Epoch #%d, train loss: %0.5f, train acc: %0.3f, test acc: %0.3f\"\n",
    "          % (epoch + 1, losses[-1], accuracies[-1], accuracies_test[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.title(\"Training loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracies, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boundary(X, pred):\n",
    "    \n",
    "    x_min, x_max = X[:,0].min() - .1, X[:,0].max() + .1\n",
    "    y_min, y_max = X[:,1].min() - .1, X[:,1].max() + .1\n",
    "    \n",
    "    xs, ys = np.meshgrid(\n",
    "        np.linspace(x_min, x_max, 200),\n",
    "        np.linspace(y_min, y_max, 200)\n",
    "    )\n",
    "\n",
    "    xys = np.column_stack([xs.ravel(), ys.ravel()])\n",
    "    zs = pred(xys).reshape(xs.shape)\n",
    "    plt.contour(xs, ys, zs, colors='black')\n",
    "\n",
    "plot_points(X_train, y_train.flatten())\n",
    "plot_boundary(X_train, lambda x: model.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batches\n",
    "\n",
    "- The current implementations of `train` and `grad_loss` function currently only accept a single sample at a time:\n",
    "    - implement MiniBatchNeuralNet class which supports training with a mini-batch of batch_size samples at a time instead of one,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatchNeuralNet(NeuralNet):\n",
    "    \"\"\"MLP with 1 hidden layer with a sigmoid activation Using Mini Batch Learning\"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - experiment with different sizes of batches,\n",
    "    - monitor the norm of the average gradients on the full training set at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part2: Digit Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 45\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(digits.images[sample_index], cmap=plt.cm.gray_r,\n",
    "           interpolation='nearest')\n",
    "plt.title(\"image label: %d\" % digits.target[sample_index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "- Normalization\n",
    "- Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = np.asarray(digits.data, dtype='float32')\n",
    "target = np.asarray(digits.target, dtype='int32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, target, test_size=0.15, random_state=37)\n",
    "\n",
    "# mean = 0 ; standard deviation = 1.0\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(scaler.mean_)\n",
    "print(scaler.scale_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 10\n",
    "n_classes = 10\n",
    "n_features = 64\n",
    "model = NeuralNet(n_features, n_hidden, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.loss(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.accuracy(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(model, sample_idx=0, classes=range(10)):\n",
    "    fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "    ax0.imshow(scaler.inverse_transform(X_test[sample_idx]).reshape(8, 8), cmap=plt.cm.gray_r,\n",
    "               interpolation='nearest')\n",
    "    ax0.set_title(\"True image label: %d\" % y_test[sample_idx]);\n",
    "\n",
    "\n",
    "    ax1.bar(classes, one_hot(len(classes), y_test[sample_idx]), label='true')\n",
    "    ax1.bar(classes, model.forward(X_test[sample_idx]), label='prediction', color=\"red\")\n",
    "    ax1.set_xticks(classes)\n",
    "    prediction = model.predict(X_test[sample_idx])\n",
    "    ax1.set_title('Output probabilities (prediction: %d)'\n",
    "                  % prediction)\n",
    "    ax1.set_xlabel('Digit class')\n",
    "    ax1.legend()\n",
    "\n",
    "plot_prediction(model, sample_idx=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, accuracies, accuracies_test = [], [], []\n",
    "losses.append(model.loss(X_train, y_train))\n",
    "accuracies.append(model.accuracy(X_train, y_train))\n",
    "accuracies_test.append(model.accuracy(X_test, y_test))\n",
    "\n",
    "print(\"Random init: train loss: %0.5f, train acc: %0.3f, test acc: %0.3f\"\n",
    "      % (losses[-1], accuracies[-1], accuracies_test[-1]))\n",
    "\n",
    "for epoch in range(15):\n",
    "    for i, (x, y) in enumerate(zip(X_train, y_train)):\n",
    "        model.train(x, y, 0.1)\n",
    "\n",
    "    losses.append(model.loss(X_train, y_train))\n",
    "    accuracies.append(model.accuracy(X_train, y_train))\n",
    "    accuracies_test.append(model.accuracy(X_test, y_test))\n",
    "    print(\"Epoch #%d, train loss: %0.5f, train acc: %0.3f, test acc: %0.3f\"\n",
    "          % (epoch + 1, losses[-1], accuracies[-1], accuracies_test[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.title(\"Training loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracies, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(model, sample_idx=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Look at worst prediction errors:\n",
    "\n",
    "    - Use numpy to find test samples for which the model made the worst predictions,\n",
    "    - Use the `plot_prediction` to look at the model predictions on those,\n",
    "    - Would you have done any better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Hyper parameters settings\n",
    "\n",
    "- Experiment with different hyper parameters:\n",
    "  - learning rate,\n",
    "  - size of hidden layer,\n",
    "  - initialization scheme: test with 0 initialization vs uniform,\n",
    "  - implement other activation functions,\n",
    "  - implement the support for a second hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to Keras\n",
    "\n",
    "- You can now use keras to implement and train the same network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "n_features = 8 * 8\n",
    "n_classes = 10\n",
    "n_hidden = 10\n",
    "\n",
    "keras_model = Sequential()\n",
    "keras_model.add(Dense(n_hidden, input_dim=n_features, activation='sigmoid'))\n",
    "keras_model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "keras_model.compile(optimizer=SGD(lr=3),\n",
    "                    loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "keras_model.fit(X_train, to_categorical(y_train), epochs=15, batch_siz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check that the Keras model can approximately reproduce the behavior of the Numpy model when using similar hyperparameter values (size of the model, type of activations, learning rate value and use of momentum);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute the negative log likelihood of a sample 42 in the test set (can use model.predict_proba);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute the average negative log-likelihood on the full test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute the average negative log-likelihood on the full training set and check that you can get the value of the loss reported by Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Is the model overfitting or underfitting? (ensure that the model has fully converged by increasing the number of epochs to 50 or more if necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
