{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Lab 01: Linear Regression, Multiple Regression and Regularised Regression <br> <small>RÃ©da DEHAK<br> 6 january 2020</small> </center>\n",
    "\n",
    "The goal of this lab is :\n",
    "    - Implement a linear regression\n",
    "    - Try different version of the gradient descent algorithm\n",
    "    - Fit generalised linear models with ridge or Lasso regularisations\n",
    "    \n",
    "We will use a data file that contain different measurements of height (variable y) of an individual sample according to the age (variable x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pickle\n",
    "fin = open('data.pkl', 'rb')\n",
    "x = pickle.load(fin)\n",
    "y = pickle.load(fin)\n",
    "fin.close()\n",
    "print(\"shape(x) : \", x.shape)\n",
    "print(\"shape(y) : \", y.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x, y, 'x')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Height')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Linear Regression\n",
    "\n",
    "We will implement a linear regression for this problem, you know that the linear \n",
    "regression assumes that the observed variable $y$ is a linear combination of the \n",
    "vector of observation $x$\n",
    "\n",
    "$$f(x) = x^TA =\\sum_{d=1}^D a_{[d]} x_{[d]}$$\n",
    "with $x_{[D]} = 1$\n",
    "\n",
    "The linear regression consists in finding the parameters $A$ which minimizes the \n",
    "quadratic error:\n",
    "$$E(A) = \\sum_{i=1}^{N}\\left(f(x_i) - y_i\\right)^2$$\n",
    "\n",
    "we will solve this problem using two different methods:\n",
    "\n",
    "**1. Exact solution:**\n",
    "\n",
    "The vector $A$ which minimize $E(A)$ is defined as follow:\n",
    "$$A = (XX^T)^{-1}X Y$$\n",
    "\n",
    "where \n",
    "$$X=\\left[\\begin{matrix}\n",
    "x_1 & x_2 & ... & x_N\\\\\n",
    "1   &  1  & ... &  1\n",
    "\\end{matrix}\\right]$$\n",
    "\n",
    "$$Y=\\left[\\begin{matrix}\n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "...\\\\\n",
    "y_N\n",
    "\\end{matrix}\\right]$$\n",
    "\n",
    "**a-** Compute the vector $A$ wich minimize $E(A)$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ...\n",
    "Y = ...\n",
    "A = ...\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b-** Plot in the same figure the training data and the straight \n",
    "line corresponding to the obtained $A$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(A, x):\n",
    "    pass\n",
    "\n",
    "\n",
    "plt.plot(x, y, 'x', c='blue')\n",
    "pred = predict(A, x)\n",
    "plt.plot(x, pred, c='red')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Height')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c-** Predict the height of a person of age 3.5 and that of age 7? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"height(3.5) = \", ...)\n",
    "print(\"height(7) = \", ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Gradient Descent :**\n",
    "In this part, we will use the gradient descent algorithm (see convex optimization course) to find the best regression parameters. We will use the batch learning. \n",
    "\n",
    "**a-** Give the recurrence formula for $A$ of the algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$ A_n = ...$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b-** Implement a gradient descent with a learning rate $\\eta = 0.035$ \n",
    "and starting from the origin of the space $A = 0$? Wait until the convergence of the algorithm? (print at each iteration, the number of iteration, the Error and the norm of the gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.zeros((2, 1))\n",
    "N = X.shape[1]\n",
    "eta = 0.035\n",
    "tol = 1e-3\n",
    "iter = 1\n",
    "while ...\n",
    "    ...\n",
    "    \n",
    "print(iter, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c-** Did you obtain the same result as question 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d-** To understand the gradient descent, we will display in 3D the curve of the error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "error = np.zeros((100,100))\n",
    "a0 = np.linspace(-.2, .45, 100)\n",
    "a1 = np.linspace(-.1, 1, 100)\n",
    "for i in range(a0.shape[0]):\n",
    "    for j in range(a1.shape[0]):\n",
    "        A = np.array([[a0[i]],[a1[j]]])\n",
    "        error[i, j] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib import interactive\n",
    "A0, A1 = np.meshgrid(a0, a1)\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(A0, A1, error.T, cmap=cm.jet, rstride=1, cstride=1)\n",
    "ax.set_xlabel('A0')\n",
    "ax.set_ylabel('A1')\n",
    "interactive(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see different views of the plot using the mouse in the interactive mode\n",
    "\n",
    "**e-** What is the link between this figure and different values of $A$ founded during the iterations of the gradient descent algorithm? Plot the path obtained using the different values of $A$ in the same figure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib import interactive\n",
    "A0, A1 = np.meshgrid(a0, a1)\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(A0, A1, error.T , cmap=cm.jet, rstride=1, cstride=1)\n",
    "ax.plot3D(... , color ='red')\n",
    "ax.set_xlabel('A0')\n",
    "ax.set_ylabel('A1')\n",
    "interactive(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f-** Conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**g-** How can we improve the speed of convergence? implement this new method and compare the result with the previous decent algorithm? Plot the path obtained using the different values of A in the same figure for the two algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib import interactive\n",
    "A0, A1 = np.meshgrid(a0, a1)\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(A0, A1, error.T , cmap=cm.jet, rstride=1, cstride=1)\n",
    "ax.plot3D(... , color ='red')\n",
    "ax.plot3D(... , color ='green')\n",
    "ax.set_xlabel('A0')\n",
    "ax.set_ylabel('A1')\n",
    "interactive(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**h-** To run a linear Regression you can also use the sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression(fit_intercept = False).fit(X.T, Y)\n",
    "print(\"A = \", reg.coef_)\n",
    "print(\"height(3.5) = \", reg.predict(np.array([[3.5, 1]])))\n",
    "print(\"height(7) = \", reg.predict(np.array([[7, 1]])))\n",
    "print(\"Score = \", reg.score(X.T, y.reshape((50,1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Regularised Regression \n",
    "### Import Data\n",
    "\n",
    "The following dataset is from Hastie, Tibshirani and Friedman (2009), from a study by Stamey et al. (1989) of prostate cancer, measuring the correlation between the level of a prostate-specific antigen and some covariates. The covariates are\n",
    "- lcavol : log-cancer volume\n",
    "-  lweight : log-prostate weight\n",
    "-  age : age of patient\n",
    "-  lbhp : log-amount of benign hyperplasia\n",
    "-  svi : seminal vesicle invasion\n",
    "-  lcp : log-capsular penetration\n",
    "-  gleason : Gleason Score,\n",
    "-  lpsa is the response variable, log-psa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = open('data2.pkl', 'rb')\n",
    "xtrain = pickle.load(fin)\n",
    "ytrain = pickle.load(fin)\n",
    "Xtest = pickle.load(fin)\n",
    "Ytest = pickle.load(fin)\n",
    "fin.close()\n",
    "\n",
    "print('Train data : ', xtrain.shape, ' ', ytrain.shape)\n",
    "print('Test data : ', Xtest.shape, ' ', Ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "Using the program of Part 1, compute the linear regression weight $w$\n",
    "\n",
    "$$y = g(x) = W^T x =\\sum_{d=0}^7 w_d x_d$$\n",
    "with $x_0 = 1$\n",
    "\n",
    "The linear regression consists in finding the parameters $W$ which minimizes the \n",
    "quadratic error:\n",
    "$$E(W) = \\frac{1}{60}\\sum_{i=1}^{60}\\left(g(x_i) - y_i\\right)^2$$\n",
    "\n",
    "The vector $W$ which minimize $E(W)$ is defined as follow:\n",
    "$$W = (X X^T)^{-1}X Y$$\n",
    "\n",
    "Compute the vector $W$ wich minimize $E(W)$ :\n",
    "- Compute $w$ using the exact solution\n",
    "- Compute the error on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check that you obtain the same $W$ with sklean.linear_model.LinearRegression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression\n",
    "\n",
    "The ridge regression consists in finding the parameters $W$ which minimizes:\n",
    "$$\\frac{1}{60}\\sum_{i=1}^{60}\\left(W^T x_i - y_i\\right)^2 + \\alpha \\|W\\|_2^2$$ \n",
    "\n",
    "- Using linear_model.Ridge and $\\alpha = 0.$, check that you obtain the same $W$ as linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will check the influence of $\\alpha$ on the solution of the linear regression\n",
    "\n",
    "- Train a ridge regression with different values of $\\alpha$ = np.logspace(-5, 5, 200), save the values of W, Mean squared errors on train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-5, 5, 200)\n",
    "e=[]\n",
    "w=[]\n",
    "for a in alphas:\n",
    "    ...\n",
    "    etrain = ...\n",
    "    etest = ...\n",
    "    e.append(np.array([etrain, etest]))\n",
    "    w.append(...)\n",
    "\n",
    "e=np.array(e).T\n",
    "w=np.array(w).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot how evolve each $W_i$ through the sequence of $\\alpha$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    plt.semilogx(alphas, w[i, :])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot how evolve the mean square error on train and test data through the sequence of $\\alpha$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Mean Squared Error on train data\n",
    "plt.semilogx(alphas, e[0, :], color = 'blue')\n",
    "# Plot Mean Squared Error on train data\n",
    "plt.semilogx(alphas, e[1, :], color = 'red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conclude? (Which is the best value for $\\alpha$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso regression\n",
    "\n",
    "The ridge regression consists in finding the parameters $W$ which minimizes:\n",
    "$$\\frac{1}{2 \\times 60}\\sum_{i=1}^{60}\\left(W^T x_i - y_i\\right)^2 + \\alpha \\|W\\|_1$$\n",
    "\n",
    "- Using linear_model.Lasso and $\\alpha = 0.$, check that you obtain the same $W$ as linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will check the influence of $\\alpha$ on the solution of the linear regression\n",
    "\n",
    "- Train a Lasso regression with different values of $\\alpha$ = np.logspace(-5, 5, 200), save the values of W, Mean squared errors on train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-5, 5, 200)\n",
    "e=[]\n",
    "w=[]\n",
    "for a in alphas:\n",
    "    ...\n",
    "    etrain = ...\n",
    "    etest = ...\n",
    "    e.append(np.array([etrain, etest]))\n",
    "    w.append(...)\n",
    "\n",
    "e=np.array(e).T\n",
    "w=np.array(w).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot how evolve each $W_i$ through the sequence of $\\alpha$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    plt.semilogx(alphas, w[i, :])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot how evolve the mean square error on train and test data through the sequence of $\\alpha$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Mean Squared Error on train data\n",
    "plt.semilogx(alphas, e[0, :], color = 'blue')\n",
    "# Plot Mean Squared Error on train data\n",
    "plt.semilogx(alphas, e[1, :], color = 'red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conclude? (Which is the best value for $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compare the result with ridge solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
