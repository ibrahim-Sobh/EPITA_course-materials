{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Auto encoders\n\nAuto-encoder and its variants are models that can be used for several things: \n\n- data compression and dimensionality reduction\n- Denoising, recolorisation or super resolution\n- anomaly detection \n\nIn this notebook we will train our first auto-encoder do to MNIST image reconstruction\n",
   "metadata": {
    "tags": [],
    "cell_id": "9808df69-8f17-4a19-9003-d25c16b73b9d",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "###  First auto-encoder without convolutions\n\n\nLoad the mnist dataset into train and test datasets, resize it  normalize them with a minmax scaler",
   "metadata": {
    "output_cleared": false,
    "cell_id": "00001-01f7aa9a-78bc-43a1-a466-883a7972cce1",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00002-946ec753-6817-4992-a349-1fd441bf5e61",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Part one the encoder: its goal is to take an original data and reduce its dimension (here to dimension=30)\n\nCreate a first model with (with functional api if keras) with the following layers : \n\n- A dense Relu layer with 300 neurones\n- A dense Relu layer with 30 neurones\n\nName it  encoder\n",
   "metadata": {
    "cell_id": "00003-2517aae4-bae3-409c-852a-b53e9d944de8",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00004-c57907ee-e7db-4950-b1b1-9d79eac7e017",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Now, the decoder. It is the part that takes the compressed data from the encoder and tries to decode it to the original image\n\n\nCreate a second model with the following layers : \n\n- Relu Dense 200\n- Sigmoid Dense 28*28. \n\nWe use sigmoid at the end because MNIST dataset pixels are either black or white. So we can treat that as a classification problem : the output just classify each pixel of the original image\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00005-959fd044-fa0f-4d37-ae19-f4e7aa18273c",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00006-52782386-2670-44db-871d-5a46e9304542",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Create an auto-encoder model which is the assembly of both the encoder and the decoder",
   "metadata": {
    "cell_id": "00007-99de1c74-0152-4467-8b0c-4ddd9d50ab78",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00008-9c3e9c4a-5345-49b2-a2e9-30ddeb6153ce",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Train the whole auto-encoder model on the training set. What are the labels that must be used for the .fit method ?",
   "metadata": {
    "output_cleared": false,
    "cell_id": "00009-f93d00ce-9fd3-4c6e-9b44-5bda4083cc3a",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution_millis": 3954,
    "execution_start": 1643121813336,
    "output_cleared": false,
    "source_hash": "6aa76cef",
    "tags": [],
    "cell_id": "00010-7830990b-2639-442f-883e-faafa991775a",
    "deepnote_to_be_reexecuted": false,
    "deepnote_cell_type": "code"
   },
   "source": "from tensorflow.keras import Sequential, Input, Model\nfrom tensorflow.keras.layers import Dense \n\n# on fait du mnist comme d'hab ! \n\n\n\ninput = Input((28**2))\n\n\nx = Dense(200, activation=\"relu\")(input)\nx = Dense(30, activation=\"relu\")(x)# the output of this layer is just a 30 dimensional vector \n\nx = Dense(200, activation=\"relu\")(x)\no = Dense(28*28, activation=\"sigmoid\")(x) #car les pixels de MNIST sont noir(0) ou blanc(1) \n\n\n\nauto_encoder = Model(inputs=input, outputs=o)\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "3df9f1e2-0f6b-4f11-84fa-e6d718c6fa24",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "db502c64",
    "execution_start": 1643123975882,
    "execution_millis": 7,
    "deepnote_cell_type": "code"
   },
   "source": "#pareil : x = Dense(30, activation=\"relu\")(x)\n\n## api fonctionnelle : utiliser une couche commme une fonction\nlayer1 = Dense(30, activation=\"relu\")\nprint(type(layer1))\nx = layer1(x)",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "<class 'tensorflow.python.keras.layers.core.Dense'>\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "b8d34050-91b9-444f-8e0e-dbae7584a39b",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e8b1e359",
    "execution_start": 1643124569155,
    "execution_millis": 331,
    "deepnote_cell_type": "code"
   },
   "source": "\nfrom tensorflow.keras.datasets.mnist import load_data\n\nmnist = load_data()\ntrain,test = mnist\n\nx_train, y_train = train \n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "f67e5840-e4cc-4555-8a5b-11dfc8b71ea2",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "3ddf6e49",
    "execution_start": 1643124573603,
    "execution_millis": 141,
    "deepnote_cell_type": "code"
   },
   "source": "# ON NE VA PAS UTILISER y_train : on ne fait pas de la classification d'image \n\n\nx_train = x_train.reshape(-1, 28**2) / 255 #(minmax scaling)\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "90594c3e-6381-4a8a-9fbe-296b357ae457",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "6b4d6ed5",
    "execution_start": 1643124574170,
    "execution_millis": 62,
    "deepnote_output_heights": [
     21
    ],
    "deepnote_cell_type": "code"
   },
   "source": "x_train.shape",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 12,
     "data": {
      "text/plain": "(60000, 784)"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "3a5a1d11-b92d-4805-bc26-c110015cd31c",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "cc4f672f",
    "execution_start": 1643126171844,
    "execution_millis": 1,
    "deepnote_output_heights": [
     21
    ],
    "deepnote_cell_type": "code"
   },
   "source": "input = Input((28**2,))\n\nx = Dense(200, activation=\"relu\")(input)\nencoder_o = Dense(30, activation=\"relu\")(x)# the output of this layer is just a 30 dimensional vector \n\nx = Dense(200, activation=\"relu\")(encoder_o)\no = Dense(28*28, activation=\"sigmoid\")(x) #car les pixels de MNIST sont noir(0) ou blanc(1) \n\n\nauto_encoder = Model(inputs=input, outputs=o)\n\nencoder =      Model(inputs=input, outputs=encoder_o)\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "0f1ba126-6640-47a1-b329-c5c9d7d76ba1",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "20c62cca",
    "execution_start": 1643126136511,
    "execution_millis": 1,
    "deepnote_cell_type": "code"
   },
   "source": "\ndef apply_decoder(auto_encoder, input):\n\n    o = auto_encoder.layers[-2](input)\n    o = auto_encoder.layers[-1](o)\n    return o\n\ninput_decoder = Input((30,))\ndecoder = Model(input_decoder, apply_decoder(auto_encoder, input_decoder))\n\n\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "222b232d-6c9e-49fb-8782-849624b02c59",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "3cc2eab4",
    "execution_start": 1643125162655,
    "execution_millis": 167,
    "deepnote_output_heights": [
     21
    ],
    "deepnote_cell_type": "code"
   },
   "source": "encoder.predict(x_train[:3]).shape",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 27,
     "data": {
      "text/plain": "(3, 30)"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "544d4f98-6d8b-4298-b6d8-6e239a34a2c0",
    "tags": [],
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "b588bb10-1ac3-4078-8745-b92a6e9c8ae2",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "5fa5ade3",
    "execution_start": 1643125114210,
    "execution_millis": 17066,
    "deepnote_cell_type": "code"
   },
   "source": "\nauto_encoder.compile(optimizer=\"adam\", loss=\"binary_crossentropy\") # l'accuracy n'est pas une mÃ©trique pertinente ici\nauto_encoder.fit(x_train, x_train, epochs=10)",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch 1/10\n1875/1875 [==============================] - 12s 6ms/step - loss: 0.1748\nEpoch 2/10\n 671/1875 [=========>....................] - ETA: 7s - loss: 0.0988",
     "output_type": "stream"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-bb6912160285>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mauto_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"binary_crossentropy\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# l'accuracy n'est pas une mÃ©trique pertinente ici\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mauto_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "aa797b4c-fcfb-42b8-8019-d2a1fa1364f2",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "6bfa3902",
    "execution_start": 1643124726386,
    "execution_millis": 888,
    "deepnote_output_heights": [
     null,
     21,
     250,
     250
    ],
    "deepnote_cell_type": "code"
   },
   "source": "X_pred = auto_encoder.predict(x_train[:3])\n\nimport matplotlib.pyplot as plt \n\nfirst_pred = X_pred[0]\nprint(first_pred.shape)\nplt.imshow(first_pred.reshape(28, 28), cmap=\"gray\")\nplt.figure()\nplt.imshow(x_train[0].reshape(28, 28), cmap=\"gray\")",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "(784,)\n",
     "output_type": "stream"
    },
    {
     "output_type": "execute_result",
     "execution_count": 19,
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7f1fd7bfd3d0>"
     },
     "metadata": {}
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light",
      "image/png": {
       "width": 251,
       "height": 248
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light",
      "image/png": {
       "width": 251,
       "height": 248
      }
     },
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "1e666c55-ed73-4aac-ab59-47b583de2a52",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "ec254d5e",
    "execution_start": 1643125490677,
    "execution_millis": 21,
    "deepnote_cell_type": "code"
   },
   "source": "import tensorflow\nlayer = tensorflow.keras.layers.UpSampling2D(\n    size=(2, 2), data_format=None, interpolation=\"nearest\"\n)\n\nx = Input((2, 2, 1))\n\ny = layer(x)\nprint(y.shape)",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "(None, 4, 4, 1)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "0dd52a99-e550-4690-9954-d4cae0439140",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "c6e2ced1",
    "execution_start": 1643117087692,
    "execution_millis": 59,
    "deepnote_cell_type": "code"
   },
   "source": "\n\nX = train \ny = X\n\nauto_encoder.fit(X, X)\n\nresult = auto_encoder.predict(X[0]) => resortira un vecteur \nplt.imshow(result.reshape(28, 28), cmap=\"gray\")",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_2\" was not an Input tensor, it was generated by layer dense_1.\nNote that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\nThe tensor that caused the issue was: dense_1/Relu:0\n",
     "output_type": "stream"
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 784), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\") at layer \"dense\". The following previous layers were accessed without issue: []",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e9b4997323ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#car les pixels de MNIST sont noir(0) ou blanc(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mauto_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mgeneric_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFunctional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     nodes, nodes_by_depth, layers, _ = _map_graph_network(\n\u001b[0;32m--> 204\u001b[0;31m         self.inputs, self.outputs)\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m    988\u001b[0m                              \u001b[0;34m'The following previous layers '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m                              \u001b[0;34m'were accessed without issue: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m                              str(layers_with_complete_input))\n\u001b[0m\u001b[1;32m    991\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0mcomputable_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 784), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\") at layer \"dense\". The following previous layers were accessed without issue: []"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "Feed some images to the model and display the reconstructed images along with the original images. \n\n",
   "metadata": {
    "cell_id": "00011-030dcc04-afe4-4b5f-8602-ee4a8c0ae049",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution_millis": 423,
    "execution_start": 1607341917454,
    "output_cleared": false,
    "source_hash": "54fdbf98",
    "cell_id": "00012-55a0aab3-6e3b-42ee-aa64-6b5dade7ea7b",
    "deepnote_cell_type": "code"
   },
   "source": "def reconstruction_error(X, X_pred):\n\n    return mse(X.reshape(-1, 1), X.reshape(-1, 1))\n\n\ndef is_anomaly(model, X, threshold):\n    X_pred = model.predict(X)\n\n    return (reconstruction_error > threshold)\n        ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## CNN based Auto-encoder\n\nWe can of course have auto-encoder with convolutions ! \n\nWe will replace\n\n- Dense layers by convolution layers in the encoder model and add some maxPooling after each convolution\n- Dense layers by Upsampling2D layers in the decoder model\n\n",
   "metadata": {
    "cell_id": "00013-baf33a00-290a-42c5-8349-36e6216a893b",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "1. Create an Upsampling2D layer with Keras\n\n2. Calculate the output of the layer and store the result in the upsampled variable on the array created above.\n\nReacreate the encoder, decoder and auto-coder model as previously",
   "metadata": {
    "output_cleared": false,
    "cell_id": "00014-9e6189e8-cc6a-40f7-ae04-41609f988d0e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution_millis": 1,
    "execution_start": 1607341930915,
    "output_cleared": false,
    "source_hash": "572a2d1f",
    "cell_id": "00015-6aeddfe7-d308-49d3-946a-39b4509fe6b4",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Reload the MNIST dataset and do the proper scaling and resizing (rember, keras convolutions expects to see a 4D array with the dimensions (nb_exemple, n_col, n_row, n_channel)",
   "metadata": {
    "cell_id": "00016-e8fbf251-6234-478c-95a4-1745b41d714d",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00017-2e0ab2a7-e983-4429-a8d3-d4d78b6e9ccc",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Train the auto-encoder model. Display some reconstructed images. Does it work better than the previous model?",
   "metadata": {
    "output_cleared": false,
    "cell_id": "00018-6afb0eea-05eb-4899-baf1-5324e3f8532b",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution_millis": 68,
    "execution_start": 1607341930916,
    "output_cleared": false,
    "source_hash": "ff241c96",
    "cell_id": "00019-2bae79b5-0157-47d4-a529-77439379442c",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Bonus : Denoising auto-encoder. \n\nLet's say that we have two version of our images : \n\n- image with noise\n- the same image without noise. \n\nWe can ask our auto-encoder to reconstruct clean images from noisy images\n\n",
   "metadata": {
    "cell_id": "00020-1c098441-1128-4a94-9c4a-bfe743e8ee17",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Load the Mnist dataset. \n",
   "metadata": {
    "cell_id": "00021-208de402-aa8d-4dfb-92ef-0b4fd84c74dd",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00022-b83f70ca-0f20-40a4-ada9-d9636f9acf82",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "With the noise function bellow, create a corrupted version of MNIST by adding some noise",
   "metadata": {
    "cell_id": "00023-d6451c15-463c-43bf-8d03-21cf00ef1f94",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00024-7cf4c44f-90b5-491f-a817-72bcb7d1f3b1",
    "deepnote_cell_type": "code"
   },
   "source": "def noise(array):\n    \"\"\"\n    Adds random noise to each image in the supplied array.\n    \"\"\"\n\n    noise_factor = 0.4\n    noisy_array = array + noise_factor * np.random.normal(\n        loc=0.0, scale=1.0, size=array.shape\n    )\n\n    return np.clip(noisy_array, 0.0, 1.0)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Display some noisy images and check you can still recognize mnist dataset ",
   "metadata": {
    "cell_id": "00025-655318a4-e160-4d69-87e7-cf78436e7e33",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00026-8bc7de0e-a366-4c2d-bdae-fbe65cc30e05",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Instanciate a new version of the CNN auto-encoder. Train it now to reconstruct clean images from noisy ones. ",
   "metadata": {
    "cell_id": "00027-b36e1012-fef0-4c2f-b333-661b54e0d284",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00028-9832c5cc-1c1b-4c31-a97d-babaeb002947",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Feed some noisy image to the network and check that it outputs some cleaned images with matplotlib",
   "metadata": {
    "cell_id": "00029-83e1b43c-f29e-4e04-8976-5b96a0b855dc",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00030-87bb92ef-a770-4f1f-9c88-3a9b9ab74458",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Search and Think about other applications of auto-encoder ",
   "metadata": {
    "cell_id": "00031-d34ed3bd-57cb-4d77-bba6-73fdd23c5ad3",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00032-0c10dd98-3be1-453d-945e-c453c289e2b7",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=6b859965-b858-4b8d-a841-009599aef86e' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "e5bb7b9c-e1e3-4388-bfac-99684ae70de4",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 }
}